{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3b40939-555a-4758-81bc-91270ab29b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from datasets import load_metric\n",
    "\n",
    "from load_fsq_model import load_model\n",
    "from modules.CustomWav2Vec2 import CustomWav2Vec2Config, CustomWav2Vec2Model\n",
    "from modules.Wav2Vec2Model import Wav2Vec2Config, Wav2Vec2Model\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6281789-a5a8-4473-b1e2-9ddd6eec823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG ------------------------------\n",
    "TEACHER_MODEL = 'wav2vec2_vox_960h_new.pt'\n",
    "DATA_PATH = '../'\n",
    "NUM_EPOCHS = 100\n",
    "GPUS = 2\n",
    "BATCH_SIZE = 2\n",
    "LEARNING_RATE = 1e-5\n",
    "ACCUMULATE_GRAD_BATCHES = 1\n",
    "OUTPUT_DIR = './results/'\n",
    "# CHECKPOINT = 'last.ckpt'\n",
    "CHECKPOINT = None\n",
    "TEST = False\n",
    "# --------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf62d2eb-8c47-4cf7-8173-79138dcab817",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac7f5fc7-18bf-420e-a53b-e93d713946a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = load_metric(\"wer\")\n",
    "cer_metric = load_metric(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f54842c3-4ea7-4077-87d8-6cac14099de3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "teacher_model = load_model(TEACHER_MODEL)\n",
    "teacher_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "961e685a-9645-4e8b-9f25-9e32e0a5fb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_tf_encoder = teacher_model.w2v_encoder.w2v_model.encoder.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92afbc75-aa21-4384-9b53-b6ac10cffbcc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Config(_name=None, conv_layer_setting=ConvFeatureExtractionModelConfig(_name=None, extractor_mode='layer_norm', conv_feature_layers='[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] * 2', conv_bias=True, drop_out=0.5), encoder_setting=TransformerEncoderConfig(_name=None, layer_setting=TransformerSentenceEncoderLayerConfig(_name=None, encoder_embed_dim=1024, encoder_ffn_embed_dim=4096, encoder_attention_heads=16, dropout=0.0, attention_dropout=0.1, activation_dropout=0.0, activation_fn='gelu', layer_norm_first=True), encoder_layers=24, conv_pos=128, conv_pos_groups=16, encoder_layerdrop=0.0), dropout_input=0.1, dropout_features=0.1, final_dim=768, logit_temp=0.1, quantize_targets=True, quantize_input=False, same_quantizer=False, target_glu=False, feature_grad_mult=1.0, quantizer_depth=1, quantizer_factor=3, latent_vars=320, latent_groups=2, latent_dim=0, mask_length=10, mask_prob=0.65, mask_selection='static', mask_other=0.0, no_mask_overlap=False, mask_min_space=1, mask_channel_length=10, mask_channel_prob=0.0, mask_channel_before=False, mask_channel_selection='static', mask_channel_other=0.0, no_mask_channel_overlap=False, mask_channel_min_space=1, num_negatives=100, negatives_from_everywhere=False, cross_sample_negatives=0, codebook_negatives=0, latent_temp=[2.0, 0.1, 0.999995])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print Teacher Model Wav2Vec2Model config\n",
    "teacher_model.w2v_encoder.w2v_model.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4fb0834-8f1c-4759-8ec2-bc5c5175832c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'student_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16839/1940802734.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Print Student Model Wav2Vec2Model config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstudent_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'student_config' is not defined"
     ]
    }
   ],
   "source": [
    "# Print Student Model Wav2Vec2Model config\n",
    "student_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd1de016-aeec-450a-9fd8-431abbd37683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze model parameters\n",
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75df171d-8059-49a2-b594-88e2e04df257",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_config = CustomWav2Vec2Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4777fcca-a8c8-48ff-918c-52cd60dadacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_config.conv_layer_setting.extractor_mode = \"layer_norm\"\n",
    "student_config.conv_bias = True\n",
    "student_config.encoder_setting.layer_setting.encoder_embed_dim = 1024\n",
    "student_config.encoder_setting.layer_setting.encoder_ffn_embed_dim = 4096\n",
    "student_config.encoder_setting.layer_setting.encoder_attention_heads = 16\n",
    "student_config.encoder_setting.layer_setting.dropout = 0.0\n",
    "student_config.encoder_setting.layer_setting.layer_norm_first=True\n",
    "student_config.encoder_setting.type_of_tr_layer = \"conv1d\"\n",
    "student_config.encoder_setting.encoder_layers = 6\n",
    "student_config.encoder_setting.tr_layer_floor = 3\n",
    "student_config.encoder_setting.dropout_input = 0.1\n",
    "student_config.encoder_setting.dropout_features = 0.1\n",
    "student_config.encoder_setting.final_dim = 768\n",
    "student_config.encoder_setting.latent_temp = (2, 0.1, 0.999995)\n",
    "student_config.final_dropout = 0.0\n",
    "student_config.targ_d = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82a1e4d9-1d07-465e-9985-140bf7982c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model = CustomStudentModel(student_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34cdd87b-2e78-44a3-8d38-7ef85c58df07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "train_data = torchaudio.datasets.LIBRISPEECH(DATA_PATH, \"train-clean-100\", download=True)\n",
    "eval_data = torchaudio.datasets.LIBRISPEECH(DATA_PATH, \"dev-clean\", download=True)\n",
    "test_data = torchaudio.datasets.LIBRISPEECH(DATA_PATH, \"test-clean\", download=True)\n",
    "# sample = test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbea11a6-da9a-48d5-b97f-e69e07411d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(train_data, batch_size=2, collate_fn=data_collator, num_workers=4)\n",
    "val_dataloader = DataLoader(eval_data, batch_size=2, collate_fn=data_collator, num_workers=4)\n",
    "test_dataloader = DataLoader(test_data, batch_size=2, collate_fn=data_collator, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8df4f1d-4ac8-45a2-af9c-2a1181592c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b42dbd4-59c1-440c-af08-46d13b7e6cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctc_converter = CTCSequenceConverter(return_type=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "331d2c74-21ce-4576-ae9c-db6ef2f3265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1loss = nn.L1Loss()\n",
    "CTCloss = nn.CTCLoss(blank=4, zero_infinity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dddd306c-1522-43de-a377-dcd627d0eb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 292, 292])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(val_dataloader):\n",
    "    teacher_results = teacher_model.w2v_encoder.w2v_model.extract_features(\n",
    "            source=batch['src'], \n",
    "            # padding_mask=batch['mask'],\n",
    "            padding_mask=None,\n",
    "            layer=100\n",
    "        )\n",
    "    print(teacher_results['layer_results'][11][1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c08875e2-16c9-4fcb-86d9-a91c8d58d221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([292, 2, 1024])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_results['layer_results'][11][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5a33e7f-a9f9-46ce-bc01-84b48ead5290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': tensor([[ 2.3804e-03,  2.0752e-03,  1.9836e-03,  ...,  4.2725e-04,\n",
      "          5.7983e-04,  1.0376e-03],\n",
      "        [-1.5259e-04, -9.1553e-05, -1.8311e-04,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]]), 'mask': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 1., 1.]]), 'labels': ['MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL', \"NOR IS MISTER QUILTER'S MANNER LESS INTERESTING THAN HIS MATTER\"]}\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(val_dataloader):\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7cadc30-b12f-4016-b971-64895b8f9b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(student_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db86249c-22f4-4c30-9711-6ec9d6c40cbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14.3856, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(val_dataloader):\n",
    "    result = teacher_model.w2v_encoder.w2v_model.extract_features(\n",
    "            source=batch['src'], \n",
    "            # padding_mask=batch['mask'],\n",
    "            padding_mask=None,\n",
    "            layer=100\n",
    "        )\n",
    "    \n",
    "    x = result['x'].transpose(0, 1)\n",
    "    x = teacher_model.w2v_encoder.proj(x)\n",
    "\n",
    "    teacher_results = {\n",
    "        \"encoder_out\": x,  # T x B x C\n",
    "        \"padding_mask\": result[\"padding_mask\"],  # B x T,\n",
    "        \"layer_results\": result[\"layer_results\"],\n",
    "    }\n",
    "    \n",
    "    student_results = student_model(batch['src'], padding_mask=None)\n",
    "    \n",
    "    x = student_results['tr_layer_results'][0].detach()\n",
    "    \n",
    "    for i, layer in enumerate(teacher_tf_encoder):\n",
    "        if i >= 12:\n",
    "            x, z = layer(x)\n",
    "\n",
    "    x = x.transpose(0, 1)\n",
    "    if teacher_model.w2v_encoder.w2v_model.encoder.layer_norm_first:\n",
    "        x = teacher_model.w2v_encoder.w2v_model.encoder.layer_norm(x)\n",
    "    x = x.transpose(0, 1)\n",
    "    teacher_tf_encoder_out = teacher_model.w2v_encoder.proj(x)\n",
    "    \n",
    "    ctc_input = student_results['encoder_out'].log_softmax(2) # -> Revise this\n",
    "    logits = teacher_results['encoder_out'].transpose(0,1)\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    fused_tokens = [ctc_converter(ids) for ids in predicted_ids]\n",
    "    target = torch.cat(fused_tokens)\n",
    "    target_lengths = torch.tensor([len(tokens) for tokens in fused_tokens]) # -> Revise this\n",
    "    \n",
    "    loss1 = L1loss(student_results['layer_results'][2][0], teacher_results['layer_results'][11][0])\n",
    "    loss2 = L1loss(student_results['encoder_out'], teacher_tf_encoder_out)\n",
    "    loss3 = CTCloss(\n",
    "                ctc_input, \n",
    "                target, \n",
    "                torch.full(size=(ctc_input.shape[1],), fill_value=ctc_input.shape[0]), # -> Revise this\n",
    "                target_lengths\n",
    "            )\n",
    "    \n",
    "    loss = loss1 + loss2 + loss3\n",
    "    \n",
    "    # optimizer.zero_grad()\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "    print(loss)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a200289f-898c-4ff6-afad-ebe1de02cdef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([90, 64])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = teacher_results['encoder_out'].transpose(0,1)\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "fused_tokens = [ctc_converter(ids) for ids in predicted_ids]\n",
    "target = torch.cat(fused_tokens)\n",
    "target_lengths = torch.tensor([len(tokens) for tokens in fused_tokens])\n",
    "target_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64d1389-0b7d-4002-af1c-143e2de3f189",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctc_input = student_results['encoder_out'].log_softmax(2) # -> Revise this\n",
    "logits = teacher_results['encoder_out'].transpose(0,1)\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "fused_tokens = [ctc_converter(ids) for ids in predicted_ids]\n",
    "target = torch.cat(fused_tokens)\n",
    "target_lengths = torch.tensor([len(tokens) for tokens in fused_tokens]) # -> Revise this\n",
    "loss = CTCloss(\n",
    "                ctc_input, \n",
    "                target, \n",
    "                torch.full(size=(ctc_input.shape[1],), fill_value=ctc_input.shape[0]), # -> Revise this\n",
    "                target_lengths\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af4afdb7-689b-4b93-ae2f-48a3d4918471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([17, 10, 12,  6,  5, 13,  4, 30, 16, 10, 15,  6,  5, 13,  4, 10, 12,  4,\n",
       "         6, 11,  5,  4,  7, 23,  8, 12,  6, 15,  5,  4,  8, 20,  4,  6, 11,  5,\n",
       "         4, 17, 10, 14, 14, 15,  5,  4, 19, 15,  7, 12, 12,  5, 12,  4,  7,  9,\n",
       "        14,  4, 18,  5,  4,  7, 13,  5,  4, 21, 15,  7, 14,  4,  6,  8,  4, 18,\n",
       "         5, 15, 19,  8, 17,  5,  4, 11, 10, 12,  4, 21,  8, 12, 23,  5, 15,  4,\n",
       "         9,  8, 13,  4, 10, 12,  4, 17, 10, 12,  6,  5, 13,  4, 30, 16, 10, 15,\n",
       "         6,  5, 13, 27, 12,  4, 17,  7,  9,  9,  5, 13,  4, 15,  5, 12, 12,  4,\n",
       "        10,  9,  6,  5, 13,  5, 12,  6, 10,  9, 21,  4,  6, 11,  7,  9,  4, 11,\n",
       "        10, 12,  4, 17,  7,  6,  6,  5, 13,  4])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "865378fb-84e3-48ca-a1e9-bb81b453da4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([461, 2, 32])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_results['encoder_out'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "880d8532-a26a-4cd1-b978-bfd58bc7e991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src': tensor([[-1.2207e-03, -8.5449e-04,  2.4414e-04,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [-6.1035e-05, -9.1553e-05, -1.2207e-04,  ...,  1.5259e-04,\n",
       "          -3.0518e-05, -9.1553e-05]]),\n",
       " 'mask': tensor([[0., 0., 0.,  ..., 1., 1., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'labels': ['AS FOR ETCHINGS THEY ARE OF TWO KINDS BRITISH AND FOREIGN',\n",
       "  'HE LAMENTS MOST BITTERLY THE DIVORCE THAT HAS BEEN MADE BETWEEN DECORATIVE ART AND WHAT WE USUALLY CALL PICTURES MAKES THE CUSTOMARY APPEAL TO THE LAST JUDGMENT AND REMINDS US THAT IN THE GREAT DAYS OF ART MICHAEL ANGELO WAS THE FURNISHING UPHOLSTERER']}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "55bf3ab2-8736-4c65-8f53-91c879f59a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_ids = np.argmax(student_results['encoder_out'].transpose(0,1).cpu().detach().numpy(), axis=-1)\n",
    "predictions = [decoder.decode(ids) for ids in predicted_ids]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee9c43a-a5c1-4ba5-88b5-aca0a5abcfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(tqdm(val_dataloader)):\n",
    "    logits = model(source=batch['src'].cuda(), padding_mask=batch['mask'])[\"encoder_out\"].transpose(0,1)\n",
    "    predicted_ids = np.argmax(logits.cpu().detach().numpy(), axis=-1)\n",
    "    predictions = [decoder.decode(ids) for ids in predicted_ids]\n",
    "    \n",
    "    # wer_.append(wer_metric.compute(predictions=predictions, references=labels))\n",
    "    # cer_.append(cer_metric.compute(predictions=predictions, references=labels))\n",
    "    wer_metric.add_batch(predictions=predictions, references=batch['labels'])\n",
    "    cer_metric.add_batch(predictions=predictions, references=batch['labels'])\n",
    "    \n",
    "wer = wer_metric.compute()\n",
    "cer = cer_metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c61f034-8e3c-4b36-b68e-8a3d01bc368e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ff16096-8533-45ff-9ca6-11bd011a976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_tokens = torch.cat([ctc_converter(ids) for ids in predicted_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81d27d5c-883f-4f21-bdcd-7515828cb678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([159])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = [ctc_converter(ids) for ids in predicted_ids]\n",
    "temp[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45dc76f4-32db-4f60-919e-e268992081d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[159, 43]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(tokens) for tokens in temp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0f2cc6a5-749b-4128-817a-87252169c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_length = torch.tensor([len(tokens) for tokens in temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6581c48-2331-4a2f-b3a4-3dc6baa45231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([159,  43])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f42b180-6dfb-40e2-8cf0-e69391bdefe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11,  5,  4, 11,  8, 23,  5, 14,  4,  6, 11,  5, 13,  5,  4, 18,  8, 16,\n",
       "        15, 14,  4, 24,  5,  4, 12,  6,  5, 18,  4, 20,  8, 13,  4, 14, 10,  9,\n",
       "         9,  5, 13,  4,  6, 16, 13,  9, 10, 23, 12,  4,  7,  9, 14,  4, 19,  7,\n",
       "        13, 13,  8,  6, 12,  4,  7,  9, 14,  4, 24, 13, 16, 10, 12,  5, 14,  4,\n",
       "        23,  8,  6,  7,  6,  8,  5, 12,  4,  7,  9, 14,  4, 20,  7,  6,  4, 17,\n",
       "        16,  6,  6,  8,  9,  4, 23, 10,  5, 19,  5, 12,  4,  6,  8,  4, 24,  5,\n",
       "         4, 15,  7, 14, 15,  5, 14,  4,  8, 16,  6,  4, 10,  9,  4,  6, 11, 10,\n",
       "        19, 26,  4, 23,  5, 23, 23,  5, 13,  5, 14,  4, 20, 15,  8, 16, 13,  4,\n",
       "        20,  7,  6,  6,  5,  9,  5, 14,  4, 12,  7, 16, 19,  5,  4, 12,  6, 16,\n",
       "        20, 20,  4, 10,  6,  4, 10,  9,  6,  8,  4, 22,  8, 16,  4, 11, 10, 12,\n",
       "         4, 24,  5, 15, 15, 22,  4, 19,  8, 16,  9, 12,  5, 15, 15,  5, 14,  4,\n",
       "        11, 10, 17,  4])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fused_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "725a643f-b5a4-4178-8a87-fd9ef43a2aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([202])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.tensor([[1,2,3],[4,5,6]])\n",
    "test.reshape(1,-1)\n",
    "fused_tokens = ctc_converter(predicted_ids)\n",
    "fused_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8dadc0ea-27b4-4c44-b95f-c789cc74d74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['H', 'E', '|', 'H', 'O', 'P', 'E', 'D', '|', 'T', 'H', 'E', 'R',\n",
       "       'E', '|', 'W', 'O', 'U', 'L', 'D', '|', 'B', 'E', '|', 'S', 'T',\n",
       "       'E', 'W', '|', 'F', 'O', 'R', '|', 'D', 'I', 'N', 'N', 'E', 'R',\n",
       "       '|', 'T', 'U', 'R', 'N', 'I', 'P', 'S', '|', 'A', 'N', 'D', '|',\n",
       "       'C', 'A', 'R', 'R', 'O', 'T', 'S', '|', 'A', 'N', 'D', '|', 'B',\n",
       "       'R', 'U', 'I', 'S', 'E', 'D', '|', 'P', 'O', 'T', 'A', 'T', 'O',\n",
       "       'E', 'S', '|', 'A', 'N', 'D', '|', 'F', 'A', 'T', '|', 'M', 'U',\n",
       "       'T', 'T', 'O', 'N', '|', 'P', 'I', 'E', 'C', 'E', 'S', '|', 'T',\n",
       "       'O', '|', 'B', 'E', '|', 'L', 'A', 'D', 'L', 'E', 'D', '|', 'O',\n",
       "       'U', 'T', '|', 'I', 'N', '|', 'T', 'H', 'I', 'C', 'K', '|', 'P',\n",
       "       'E', 'P', 'P', 'E', 'R', 'E', 'D', '|', 'F', 'L', 'O', 'U', 'R',\n",
       "       '|', 'F', 'A', 'T', 'T', 'E', 'N', 'E', 'D', '|', 'S', 'A', 'U',\n",
       "       'C', 'E', '|', 'S', 'T', 'U', 'F', 'F', '|', 'I', 'T', '|', 'I',\n",
       "       'N', 'T', 'O', '|', 'Y', 'O', 'U', '|', 'H', 'I', 'S', '|', 'B',\n",
       "       'E', 'L', 'L', 'Y', '|', 'C', 'O', 'U', 'N', 'S', 'E', 'L', 'L',\n",
       "       'E', 'D', '|', 'H', 'I', 'M', '|'], dtype='<U5')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict = {\"<s>\": 0, \"<pad>\": 1, \"</s>\": 2, \"<unk>\": 3, \"|\": 4, \"E\": 5, \n",
    "    \"T\": 6, \"A\": 7, \"O\": 8, \"N\": 9, \"I\": 10, \"H\": 11, \"S\": 12, \n",
    "    \"R\": 13, \"D\": 14, \"L\": 15, \"U\": 16, \"M\": 17, \"W\": 18, \"C\": 19, \n",
    "    \"F\": 20, \"G\": 21, \"Y\": 22, \"P\": 23, \"B\": 24, \"V\": 25, \"K\": 26, \n",
    "    \"'\": 27, \"X\": 28, \"J\": 29, \"Q\": 30, \"Z\": 31}\n",
    "\n",
    "look_up = np.asarray(list(dict.keys()))\n",
    "look_up[fused_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e510dee-3c2a-438a-8298-3e49ac42e9b7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44e579e-f680-4fb9-a0c1-d02d8f886634",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logits = teacher_model(source=sample[0], padding_mask=None)[\"encoder_out\"].transpose(0,1)\n",
    "\n",
    "# --------------------------------------------------------------- #\n",
    "predicted_ids = np.argmax(logits.cpu().detach().numpy(), axis=-1)\n",
    "predictions = [decoder.decode(ids) for ids in predicted_ids]\n",
    "fused_tokens = [tok[0] for tok in groupby(predicted_ids[0]) if tok[0] != 0]\n",
    "\n",
    "print(logits)\n",
    "print(logits.shape)\n",
    "print(predictions)\n",
    "print(fused_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2b402609-38f0-43ce-819a-d2f692ebab81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 15.3397, -13.0896, -13.1977,  ...,  -5.5268,  -5.8605,  -4.5874],\n",
      "         [ 15.4613, -13.2443, -13.3591,  ...,  -5.6072,  -5.9839,  -4.6637],\n",
      "         [ 15.2651, -12.9915, -13.0938,  ...,  -5.4673,  -5.7874,  -4.5339],\n",
      "         ...,\n",
      "         [ 15.1254, -12.8308, -12.9267,  ...,  -5.3783,  -5.6602,  -4.4518],\n",
      "         [ 15.0574, -12.7499, -12.8424,  ...,  -5.3364,  -5.6016,  -4.4117],\n",
      "         [ 14.9019, -12.5752, -12.6623,  ...,  -5.2408,  -5.4742,  -4.3249]]])\n",
      "torch.Size([1, 521, 32])\n",
      "['HE HOPED THERE WOULD BE STEW FOR DINNER TURNIPS AND CARROTS AND BRUISED POTATOES AND FAT MUTTON PIECES TO BE LADLED OUT IN THICK PEPPERED FLOUR FATTENED SAUCE']\n",
      "[11, 5, 4, 11, 8, 23, 5, 14, 4, 6, 11, 5, 13, 5, 4, 18, 8, 16, 15, 14, 4, 24, 5, 4, 12, 6, 5, 18, 4, 20, 8, 13, 4, 14, 10, 9, 9, 5, 13, 4, 6, 16, 13, 9, 10, 23, 12, 4, 7, 9, 14, 4, 19, 7, 13, 13, 8, 6, 12, 4, 7, 9, 14, 4, 24, 13, 16, 10, 12, 5, 14, 4, 23, 8, 6, 7, 6, 8, 5, 12, 4, 7, 9, 14, 4, 20, 7, 6, 4, 17, 16, 6, 6, 8, 9, 4, 23, 10, 5, 19, 5, 12, 4, 6, 8, 4, 24, 5, 4, 15, 7, 14, 15, 5, 14, 4, 8, 16, 6, 4, 10, 9, 4, 6, 11, 10, 19, 26, 4, 23, 5, 23, 23, 5, 13, 5, 14, 4, 20, 15, 8, 16, 13, 4, 20, 7, 6, 6, 5, 9, 5, 14, 4, 12, 7, 16, 19, 5, 4]\n"
     ]
    }
   ],
   "source": [
    "result = teacher_model.w2v_encoder.w2v_model.extract_features(\n",
    "    source=sample[0], \n",
    "    padding_mask=None,\n",
    "    layer=100\n",
    ")\n",
    "\n",
    "x = result['x'].transpose(0, 1)\n",
    "x = teacher_model.w2v_encoder.proj(x)\n",
    "\n",
    "teacher_results = {\n",
    "    \"encoder_out\": x,  # T x B x C\n",
    "    \"padding_mask\": result[\"padding_mask\"],  # B x T,\n",
    "    \"layer_results\": result[\"layer_results\"],\n",
    "}\n",
    "\n",
    "logits = teacher_results['encoder_out'].transpose(0,1)\n",
    "\n",
    "# --------------------------------------------------------------- #\n",
    "predicted_ids = np.argmax(logits.cpu().detach().numpy(), axis=-1)\n",
    "predictions = [decoder.decode(ids) for ids in predicted_ids]\n",
    "fused_tokens = [tok[0] for tok in groupby(predicted_ids[0]) if tok[0] != 0]\n",
    "\n",
    "print(logits_to_match_with)\n",
    "print(logits_to_match_with.shape)\n",
    "print(predictions)\n",
    "print(fused_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "784f113c-f208-4040-a9c6-d72d245611fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 15.3397, -13.0896, -13.1977,  ...,  -5.5268,  -5.8605,  -4.5874],\n",
      "         [ 15.4613, -13.2443, -13.3591,  ...,  -5.6072,  -5.9839,  -4.6637],\n",
      "         [ 15.2651, -12.9915, -13.0938,  ...,  -5.4673,  -5.7874,  -4.5339],\n",
      "         ...,\n",
      "         [ 15.1254, -12.8308, -12.9267,  ...,  -5.3783,  -5.6602,  -4.4518],\n",
      "         [ 15.0574, -12.7499, -12.8424,  ...,  -5.3364,  -5.6016,  -4.4117],\n",
      "         [ 14.9019, -12.5752, -12.6623,  ...,  -5.2408,  -5.4742,  -4.3249]]])\n",
      "torch.Size([1, 521, 32])\n",
      "torch.Size([1, 521])\n",
      "tensor([11,  5,  4, 11,  8, 23,  5, 14,  4,  6, 11,  5, 13,  5,  4, 18,  8, 16,\n",
      "        15, 14,  4, 24,  5,  4, 12,  6,  5, 18,  4, 20,  8, 13,  4, 14, 10,  9,\n",
      "         9,  5, 13,  4,  6, 16, 13,  9, 10, 23, 12,  4,  7,  9, 14,  4, 19,  7,\n",
      "        13, 13,  8,  6, 12,  4,  7,  9, 14,  4, 24, 13, 16, 10, 12,  5, 14,  4,\n",
      "        23,  8,  6,  7,  6,  8,  5, 12,  4,  7,  9, 14,  4, 20,  7,  6,  4, 17,\n",
      "        16,  6,  6,  8,  9,  4, 23, 10,  5, 19,  5, 12,  4,  6,  8,  4, 24,  5,\n",
      "         4, 15,  7, 14, 15,  5, 14,  4,  8, 16,  6,  4, 10,  9,  4,  6, 11, 10,\n",
      "        19, 26,  4, 23,  5, 23, 23,  5, 13,  5, 14,  4, 20, 15,  8, 16, 13,  4,\n",
      "        20,  7,  6,  6,  5,  9,  5, 14,  4, 12,  7, 16, 19,  5,  4])\n"
     ]
    }
   ],
   "source": [
    "# Torch Version\n",
    "result = teacher_model.w2v_encoder.w2v_model.extract_features(\n",
    "    source=sample[0], \n",
    "    padding_mask=None,\n",
    "    layer=100\n",
    ")\n",
    "\n",
    "x = result['x'].transpose(0, 1)\n",
    "x = teacher_model.w2v_encoder.proj(x)\n",
    "\n",
    "teacher_results = {\n",
    "    \"encoder_out\": x,  # T x B x C\n",
    "    \"padding_mask\": result[\"padding_mask\"],  # B x T,\n",
    "    \"layer_results\": result[\"layer_results\"],\n",
    "}\n",
    "\n",
    "logits = teacher_results['encoder_out'].transpose(0,1)\n",
    "\n",
    "# --------------------------------------------------------------- #\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "# fused_tokens = torch.tensor([tok[0] for tok in groupby(predicted_ids[0]) if tok[0] != 0])\n",
    "fused_tokens = converter(predicted_ids)\n",
    "\n",
    "print(logits_to_match_with)\n",
    "print(logits_to_match_with.shape)\n",
    "print(predicted_ids.shape)\n",
    "# print(predictions)\n",
    "print(fused_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebc8929-c909-4f26-93e0-770499810651",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2685c869-a321-4ce1-aa6a-e37ed0f56fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_results = student_model(sample[0], padding_mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ddab6cb9-9cc2-45c4-bb5d-e14642c20824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['encoder_out', 'padding_mask', 'layer_results', 'tr_layer_results'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1193227a-5166-4705-9405-15dd602a1fb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 14.1960,  -5.6175,   2.2727,  ...,  -8.1675,   0.7943, -14.7274]],\n",
       "\n",
       "        [[ 11.7198,  -7.6046,   3.0378,  ...,  -3.9107,   1.5182,  -4.7468]],\n",
       "\n",
       "        [[ 10.9948,   2.3512,   0.1073,  ...,  -4.7215,   1.0011,  -2.7422]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 10.2565,  -3.1519,  -1.8599,  ...,  -1.9192,  -5.8508,  -0.1338]],\n",
       "\n",
       "        [[  7.1228,   4.6386,  -5.1138,  ...,   2.3087,  -9.0622,   5.4171]],\n",
       "\n",
       "        [[  9.7594,   1.1248,  -1.2406,  ...,   6.0834, -13.6151,   1.1677]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = teacher_results['layer_results'][11][0]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f87834a2-2116-43f3-b22f-c7955f154462",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.1155,  1.4240,  0.8586,  ...,  2.0274, -0.6959, -0.2633]],\n",
       "\n",
       "        [[-2.0062,  0.4673,  0.3084,  ...,  1.1150, -0.8888, -0.6580]],\n",
       "\n",
       "        [[ 0.5156,  0.9136, -0.3271,  ...,  1.7274, -1.0569, -0.4347]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.2336,  0.0499, -0.7006,  ...,  1.2007, -0.9971, -0.9232]],\n",
       "\n",
       "        [[-1.2208, -0.1211, -0.3233,  ...,  1.3017,  0.3959,  0.3928]],\n",
       "\n",
       "        [[-2.2006, -0.5248,  0.7346,  ...,  0.8345, -0.1040,  0.3916]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_results['layer_results'][2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b59b5b17-f1b3-40a9-8b32-68cf4d533268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.3655, grad_fn=<L1LossBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L1loss = nn.L1Loss()\n",
    "loss1 = L1loss(student_results['layer_results'][2][0], teacher_results['layer_results'][11][0])\n",
    "loss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "df5995fe-89a9-40aa-8e0a-74484d03bf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = student_results['tr_layer_results'][0].detach()\n",
    "\n",
    "for i, layer in enumerate(teacher_tf_encoder):\n",
    "    if i >= 12:\n",
    "        x, z = layer(x)\n",
    "        \n",
    "x = x.transpose(0, 1)\n",
    "if teacher_model.w2v_encoder.w2v_model.encoder.layer_norm_first:\n",
    "    x = teacher_model.w2v_encoder.w2v_model.encoder.layer_norm(x)\n",
    "\n",
    "x = x.transpose(0, 1)\n",
    "teacher_tf_encoder_out = teacher_model.w2v_encoder.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "81e4cfab-2352-4ec4-ad1a-072708c970a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([260, 1, 32])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_tf_encoder_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e3f6dae8-3aac-472d-a1fd-c3182632a967",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  5.8814, -11.0086, -11.1685,  ...,  -3.4234,  -3.9079,  -3.7239]],\n",
       "\n",
       "        [[  5.9002, -11.1906, -11.3423,  ...,  -3.4618,  -3.9210,  -3.7639]],\n",
       "\n",
       "        [[  5.9180, -11.0942, -11.2541,  ...,  -3.4218,  -4.0035,  -3.7718]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  5.9409, -10.8936, -11.0562,  ...,  -3.3947,  -3.8838,  -3.6830]],\n",
       "\n",
       "        [[  5.9506, -11.0034, -11.1554,  ...,  -3.3877,  -3.9427,  -3.7168]],\n",
       "\n",
       "        [[  5.9455, -10.9126, -11.0684,  ...,  -3.4173,  -3.9037,  -3.6471]]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_tf_encoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b28f215c-1144-43ec-a0a5-d1176c87acb5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0133,  1.7132,  1.4479,  ...,  2.5222, -0.2823, -1.1558]],\n",
       "\n",
       "        [[-1.6635,  0.0967,  1.3286,  ..., -0.5803, -0.0128,  1.4295]],\n",
       "\n",
       "        [[-0.9927,  0.9092,  0.5922,  ...,  2.5229,  0.0198, -0.1499]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.5080,  2.6394,  0.2370,  ..., -2.1678, -1.7372,  0.0479]],\n",
       "\n",
       "        [[-0.7190,  1.7406,  2.1662,  ...,  2.8859,  0.7594, -2.0479]],\n",
       "\n",
       "        [[-1.4107,  1.5948,  0.2705,  ..., -0.8034,  1.7776, -2.7136]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_results['encoder_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9011e4cb-c9dc-4138-af85-41551c4bf1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([260, 1, 32])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_results['encoder_out'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d68e341a-eaaf-4e90-a2c9-3c0a1038ffa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.0481, grad_fn=<L1LossBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss2 = L1loss(student_results['encoder_out'], teacher_tf_encoder_out)\n",
    "loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "181b842e-4b0f-4832-b9e4-1d590a814118",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctc_input = student_results['encoder_out'].log_softmax(2) # -> Revise this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5d16b16a-4af5-41b8-881c-8a40987a8517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-2.3118, -2.5565, -1.5400,  ..., -5.7079, -4.2700, -5.0472]],\n",
      "\n",
      "        [[-3.5404, -4.1167, -3.4455,  ..., -3.4902, -4.2522, -6.0133]],\n",
      "\n",
      "        [[-3.6018, -5.2308, -1.9200,  ..., -5.4679, -5.6073, -6.5199]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-7.0460, -6.1014, -2.2327,  ..., -5.6954, -5.0860, -5.1001]],\n",
      "\n",
      "        [[-4.5830, -4.4180, -2.2562,  ..., -4.4676, -4.3130, -5.1296]],\n",
      "\n",
      "        [[-1.5732, -4.0880, -3.9460,  ..., -4.4798, -4.6704, -5.5060]]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([260, 1, 32])\n"
     ]
    }
   ],
   "source": [
    "print(ctc_input)\n",
    "print(ctc_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b0edea4a-6e78-4e10-a6b0-88339a23c56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11,  5,  4, 11,  8, 23,  5, 14,  4,  6, 11,  5, 13,  5,  4, 18,  8, 16,\n",
       "        15, 14,  4, 24,  5,  4, 12,  6,  5, 18,  4, 20,  8, 13,  4, 14, 10,  9,\n",
       "         9,  5, 13,  4,  6, 16, 13,  9, 10, 23, 12,  4,  7,  9, 14,  4, 19,  7,\n",
       "        13, 13,  8,  6, 12,  4,  7,  9, 14,  4, 24, 13, 16, 10, 12,  5, 14,  4,\n",
       "        23,  8,  6,  7,  6,  8,  5, 12,  4,  7,  9, 14,  4, 20,  7,  6,  4, 17,\n",
       "        16,  6,  6,  8,  9,  4, 23, 10,  5, 19,  5, 12,  4,  6,  8,  4, 24,  5,\n",
       "         4, 15,  7, 14, 15,  5, 14,  4,  8, 16,  6,  4, 10,  9,  4,  6, 11, 10,\n",
       "        19, 26,  4, 23,  5, 23, 23,  5, 13,  5, 14,  4, 20, 15,  8, 16, 13,  4,\n",
       "        20,  7,  6,  6,  5,  9,  5, 14,  4, 12,  7, 16, 19,  5,  4])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fused_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cb248175-2e7d-4b61-9a70-d31802e95872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([159])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fused_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "de5ba520-2f28-440c-98eb-b043981f348c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([260, 1, 32])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctc_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d0e62d4e-fd0b-4b7a-9501-19df9b9debc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.0094, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CTCloss = nn.CTCLoss(blank=4)\n",
    "loss3 = CTCloss(\n",
    "    ctc_input, \n",
    "    fused_tokens, \n",
    "    torch.full(size=(ctc_input.shape[1],), fill_value=ctc_input.shape[0]),\n",
    "    # torch.tensor(\n",
    "    torch.tensor(fused_tokens.shape)\n",
    ")\n",
    "loss3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b1650b2-55ee-434e-a028-dd77bf504e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HE HOPED THERE WOULD BE STEW FOR DINNER TURNIPS AND CARROTS AND BRUISED POTATOES AND FAT MUTTON PIECES TO BE LADLED OUT IN THICK PEPPERED FLOUR FATTENED SAUCE ']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = teacher_model(source=sample[0], padding_mask=None)[\"encoder_out\"].transpose(0,1)\n",
    "predicted_ids = np.argmax(logits.cpu().detach().numpy(), axis=-1)\n",
    "predictions = [decoder.decode(ids) for ids in predicted_ids]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c49beaf-38cd-4ee9-be9c-94e6e57e57b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  2304.1094, 108910.8281, 109119.4531,  ...,  25060.8281,\n",
       "           47617.1797,  40467.1797],\n",
       "         [  3238.6797, 106824.4141, 107127.7812,  ...,  24026.8047,\n",
       "           46491.0977,  39706.0820],\n",
       "         [  1863.9941, 109858.3047, 110005.1875,  ...,  25653.4727,\n",
       "           47935.5312,  40887.2773],\n",
       "         ...,\n",
       "         [   984.2344, 111826.3984, 111824.1562,  ...,  26681.4766,\n",
       "           48915.2578,  41566.8203],\n",
       "         [   531.6680, 112751.0469, 112670.9688,  ...,  27177.0664,\n",
       "           49357.2578,  41902.7656],\n",
       "         [  -493.8281, 114384.1719, 114133.1328,  ...,  28157.3457,\n",
       "           50213.3320,  42554.9023]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_logits = teacher_results['encoder_out'].transpose(0,1)\n",
    "predicted_ids = np.argmax(t_logits.cpu().detach().numpy(), axis=-1)\n",
    "predictions = [decoder.decode(ids) for ids in predicted_ids]\n",
    "t_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "49af2419-c80c-4cce-b145-bce5eb6d9942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 521)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicted_ids = torch.argmax(teacher_results['encoder_out'], dim=-1)\n",
    "predicted_ids = np.argmax(teacher_results['encoder_out'].transpose(0,1).cpu().detach().numpy(), axis=-1)\n",
    "predicted_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e863bc-ec45-44bb-8333-e36df5680620",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctc_target = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
