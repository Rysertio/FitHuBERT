{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3778bc7-9268-448b-bd0b-c7fcdde84487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Tuple\n",
    "\n",
    "from fairseq.dataclass import ChoiceEnum, FairseqDataclass\n",
    "from fairseq.modules import (\n",
    "    Fp32GroupNorm,\n",
    "    Fp32LayerNorm,\n",
    "    GradMultiply,\n",
    "    GumbelVectorQuantizer,\n",
    "    LayerNorm,\n",
    "    MultiheadAttention,\n",
    "    SamePad,\n",
    "    TransposeLast,\n",
    ")\n",
    "from fairseq import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db949e43-8180-4ccb-8a41-3122f22c876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerSentenceEncoderLayerConfig(FairseqDataclass):\n",
    "    encoder_embed_dim: int = field(\n",
    "        default=768,\n",
    "        metadata={\"help\": \"encoder embedding dimension\"}\n",
    "    )\n",
    "    \n",
    "    encoder_ffn_embed_dim: int = field(\n",
    "        default=3072,\n",
    "        metadata={\"help\": \"encoder embedding dimension for FFN\"}\n",
    "    )\n",
    "    \n",
    "    encoder_attention_heads: int = field(\n",
    "        default=12, # 8 for model init code\n",
    "        metadata={\"help\": \"num encoder attention heads\"}\n",
    "    )\n",
    "    \n",
    "    dropout: float = field(\n",
    "        default=0.1,\n",
    "        metadata={\"help\": \"dropout probability for the transformer\"}\n",
    "    )\n",
    "    \n",
    "    attention_dropout: float = field(\n",
    "        default=0.1,\n",
    "        metadata={\"help\": \"dropout probability for attention weights\"}\n",
    "    )\n",
    "    \n",
    "    activation_dropout: float = field(\n",
    "        default=0.0, # 0.1 for model init code\n",
    "        metadata={\"help\": \"dropout probability after activation in FFN\"}\n",
    "    )\n",
    "    \n",
    "    activation_fn: ChoiceEnum(utils.get_available_activation_fns()) = field(\n",
    "        default=\"gelu\", # relu for model init code\n",
    "        metadata={\"help\": \"activation function to use\"}\n",
    "        # relu, gelu, gelu_fast, gelu_accurate, tanh, linear\n",
    "    )\n",
    "    \n",
    "    layer_norm_first: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"apply layernorm first in the transformer\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2552adea-2dd6-49dc-9bb7-a85198613700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerSentenceEncoderLayerConfig(_name=None, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_attention_heads=12, dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, activation_fn='gelu', layer_norm_first=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = TransformerSentenceEncoderLayerConfig()\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8097df6-6371-418f-a30b-5c42aa1efee6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerSentenceEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a Transformer Encoder Layer used in BERT/XLM style pre-trained\n",
    "    models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: TransformerSentenceEncoderLayerConfig\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        # Initialize parameters\n",
    "        self.embedding_dim = cfg.encoder_embed_dim\n",
    "        dropout = cfg.dropout\n",
    "        self.activation_dropout = cfg.activation_dropout\n",
    "        self.layer_norm_first = cfg.layer_norm_first\n",
    "        num_attention_heads = cfg.encoder_attention_heads\n",
    "        attention_dropout = cfg.attention_dropout\n",
    "        activation_fn = cfg.activation_fn\n",
    "        ffn_embedding_dim = cfg.encoder_ffn_embed_dim\n",
    "        \n",
    "        # Initialize blocks\n",
    "        self.activation_fn = utils.get_activation_fn(activation_fn)\n",
    "        self.self_attn = MultiheadAttention(\n",
    "            self.embedding_dim,\n",
    "            num_attention_heads,\n",
    "            dropout=attention_dropout,\n",
    "            self_attention=True,\n",
    "        )\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(self.activation_dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        # layer norm associated with the self attention layer\n",
    "        self.self_attn_layer_norm = LayerNorm(self.embedding_dim)\n",
    "        self.fc1 = nn.Linear(self.embedding_dim, ffn_embedding_dim)\n",
    "        self.fc2 = nn.Linear(ffn_embedding_dim, self.embedding_dim)\n",
    "\n",
    "        # layer norm associated with the position wise feed-forward NN\n",
    "        self.final_layer_norm = LayerNorm(self.embedding_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        self_attn_mask: torch.Tensor = None,\n",
    "        self_attn_padding_mask: torch.Tensor = None,\n",
    "        need_weights: bool = False,\n",
    "        att_args=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        LayerNorm is applied either before or after the self-attention/ffn\n",
    "        modules similar to the original Transformer imlementation.\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "\n",
    "        if self.layer_norm_first:\n",
    "            x = self.self_attn_layer_norm(x)\n",
    "            x, attn = self.self_attn(\n",
    "                query=x,\n",
    "                key=x,\n",
    "                value=x,\n",
    "                key_padding_mask=self_attn_padding_mask,\n",
    "                attn_mask=self_attn_mask,\n",
    "            )\n",
    "            x = self.dropout1(x)\n",
    "            x = residual + x\n",
    "\n",
    "            residual = x\n",
    "            x = self.final_layer_norm(x)\n",
    "            x = self.activation_fn(self.fc1(x))\n",
    "            x = self.dropout2(x)\n",
    "            x = self.fc2(x)\n",
    "            x = self.dropout3(x)\n",
    "            x = residual + x\n",
    "        else:\n",
    "            x, attn = self.self_attn(\n",
    "                query=x,\n",
    "                key=x,\n",
    "                value=x,\n",
    "                key_padding_mask=self_attn_padding_mask,\n",
    "            )\n",
    "\n",
    "            x = self.dropout1(x)\n",
    "            x = residual + x\n",
    "\n",
    "            x = self.self_attn_layer_norm(x)\n",
    "\n",
    "            residual = x\n",
    "            x = self.activation_fn(self.fc1(x))\n",
    "            x = self.dropout2(x)\n",
    "            x = self.fc2(x)\n",
    "            x = self.dropout3(x)\n",
    "            x = residual + x\n",
    "            x = self.final_layer_norm(x)\n",
    "\n",
    "        return x, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f7942bd8-8ae1-45ab-9bb0-839a3d90a594",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerSentenceEncoderLayer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9da31964-606a-48f6-a7bc-389fe68f70d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerSentenceEncoderLayer(\n",
       "  (self_attn): MultiheadAttention(\n",
       "    (dropout_module): FairseqDropout()\n",
       "    (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (dropout2): Dropout(p=0.0, inplace=False)\n",
       "  (dropout3): Dropout(p=0.1, inplace=False)\n",
       "  (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "87a743ab-6ffc-498c-b518-0a472ae92fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "TransformerSentenceEncoderLayer          --                        --\n",
       "├─MultiheadAttention: 1-1                [1, 312, 768]             --\n",
       "├─Dropout: 1-2                           [1, 312, 768]             --\n",
       "├─FusedLayerNorm: 1-3                    [1, 312, 768]             1,536\n",
       "├─Linear: 1-4                            [1, 312, 3072]            2,362,368\n",
       "├─Dropout: 1-5                           [1, 312, 3072]            --\n",
       "├─Linear: 1-6                            [1, 312, 768]             2,360,064\n",
       "├─Dropout: 1-7                           [1, 312, 768]             --\n",
       "├─FusedLayerNorm: 1-8                    [1, 312, 768]             1,536\n",
       "==========================================================================================\n",
       "Total params: 4,725,504\n",
       "Trainable params: 4,725,504\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 4.73\n",
       "==========================================================================================\n",
       "Input size (MB): 0.96\n",
       "Forward/backward pass size (MB): 13.42\n",
       "Params size (MB): 18.90\n",
       "Estimated Total Size (MB): 33.28\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, (1, 312, 768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25ad44a-a3b5-4035-8652-0eb70c276600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
