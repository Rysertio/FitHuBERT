{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fd782642-35a3-461f-b038-8e03f2aa544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import contextlib\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass, field\n",
    "from omegaconf import MISSING, II, open_dict\n",
    "from typing import Any, Optional\n",
    "\n",
    "from fairseq import checkpoint_utils, tasks, utils\n",
    "from fairseq.dataclass import FairseqDataclass\n",
    "from fairseq.dataclass.utils import convert_namespace_to_omegaconf\n",
    "from fairseq.tasks import FairseqTask\n",
    "from fairseq.models import (\n",
    "    BaseFairseqModel,\n",
    "    FairseqEncoder,\n",
    "    FairseqEncoderDecoderModel,\n",
    "    FairseqIncrementalDecoder,\n",
    "    register_model,\n",
    ")\n",
    "from fairseq.models.wav2vec.wav2vec2 import MASKING_DISTRIBUTION_CHOICES\n",
    "from fairseq.modules import (\n",
    "    LayerNorm,\n",
    "    PositionalEmbedding,\n",
    "    TransformerDecoderLayer,\n",
    ")\n",
    "\n",
    "from Wav2Vec2Model import Wav2Vec2Config, Wav2Vec2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "47df190a-198f-4714-a15d-3463796f5ffc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_to_custom_config(cfg):\n",
    "    # Input : cfg; Config for wav2vec2 model\n",
    "    config = Wav2Vec2Config()\n",
    "    \n",
    "    conv_layer_config = config.conv_layer_setting\n",
    "    encoder_config = config.encoder_setting\n",
    "    encoder_layer_config = encoder_config.layer_setting\n",
    "    \n",
    "    # Feature Extractor Config\n",
    "    conv_layer_config.extractor_mode = cfg.extractor_mode\n",
    "    conv_layer_config.conv_feature_layers = cfg.conv_feature_layers\n",
    "    conv_layer_config.conv_bias = cfg.conv_bias\n",
    "    conv_layer_config.conv_dropout = 0.0 # by default\n",
    "    \n",
    "    # Encoder Layer each Config\n",
    "    encoder_layer_config.encoder_embed_dim = cfg.encoder_embed_dim\n",
    "    encoder_layer_config.encoder_ffn_embed_dim = cfg.encoder_ffn_embed_dim\n",
    "    encoder_layer_config.encoder_attention_heads = cfg.encoder_attention_heads\n",
    "    encoder_layer_config.dropout = cfg.dropout\n",
    "    encoder_layer_config.attention_dropout = cfg.attention_dropout\n",
    "    encoder_layer_config.activation_dropout = cfg.activation_dropout\n",
    "    encoder_layer_config.activation_fn = cfg.activation_fn\n",
    "    encoder_layer_config.layer_norm_first = cfg.layer_norm_first\n",
    "    \n",
    "    # Encoder Config\n",
    "    encoder_config.layer_setting = encoder_layer_config\n",
    "    encoder_config.encoder_layers = cfg.encoder_layers\n",
    "    encoder_config.conv_pos = cfg.conv_pos\n",
    "    encoder_config.conv_pos_groups = cfg.conv_pos_groups\n",
    "    encoder_config.encoder_layerdrop = cfg.encoder_layerdrop\n",
    "    \n",
    "    # Wav2vec2 Model Config\n",
    "    config.conv_layer_setting = conv_layer_config\n",
    "    config.encoder_setting = encoder_config\n",
    "    config.dropout_input = cfg.dropout_input\n",
    "    config.dropout_features = cfg.dropout_features\n",
    "    config.final_dim = cfg.final_dim\n",
    "    config.logit_temp = cfg.logit_temp\n",
    "    config.quantize_targets = cfg.quantize_targets\n",
    "    config.quantize_input = cfg.quantize_input\n",
    "    config.same_quantizer = cfg.same_quantizer\n",
    "    config.target_glu = cfg.target_glu\n",
    "    config.feature_grad_mult = cfg.feature_grad_mult\n",
    "    config.quantizer_depth = cfg.quantizer_depth\n",
    "    config.quantizer_factor = cfg.quantizer_factor\n",
    "    config.latent_vars = cfg.latent_vars\n",
    "    config.latent_groups = cfg.latent_groups\n",
    "    config.latent_dim = cfg.latent_dim\n",
    "    config.mask_length = cfg.mask_length\n",
    "    config.mask_prob = cfg.mask_prob\n",
    "    config.mask_selection = cfg.mask_selection\n",
    "    config.mask_other = cfg.mask_other\n",
    "    config.no_mask_overlap = cfg.no_mask_overlap\n",
    "    config.mask_channel_length = cfg.mask_channel_length\n",
    "    config.mask_min_space = cfg.mask_min_space\n",
    "    config.mask_channel_prob = cfg.mask_channel_prob\n",
    "    config.mask_channel_before = cfg.mask_channel_before\n",
    "    config.mask_channel_selection = cfg.mask_channel_selection\n",
    "    config.mask_channel_other = cfg.mask_channel_other\n",
    "    config.no_mask_channel_overlap = cfg.no_mask_channel_overlap\n",
    "    config.mask_channel_min_space = cfg.mask_channel_min_space\n",
    "    config.num_negatives = cfg.num_negatives\n",
    "    config.negatives_from_everywhere = cfg.negatives_from_everywhere\n",
    "    config.cross_sample_negatives = cfg.cross_sample_negatives\n",
    "    config.codebook_negatives = cfg.codebook_negatives\n",
    "    config.latent_temp = cfg.latent_temp\n",
    "    \n",
    "    return config\n",
    "\n",
    "def Embedding(num_embeddings, embedding_dim, padding_idx):\n",
    "    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n",
    "    nn.init.normal_(m.weight, mean=0, std=embedding_dim ** -0.5)\n",
    "    nn.init.constant_(m.weight[padding_idx], 0)\n",
    "    return m\n",
    "\n",
    "\n",
    "def Linear(in_features, out_features, bias=True):\n",
    "    m = nn.Linear(in_features, out_features, bias)\n",
    "    nn.init.xavier_uniform_(m.weight)\n",
    "    if bias:\n",
    "        nn.init.constant_(m.bias, 0.0)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9157324c-289e-4a0f-b651-3571d10179da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Wav2Vec2AsrConfig(FairseqDataclass):\n",
    "    # Parameter settings for fine-tuning\n",
    "    w2v_path: str = field(\n",
    "        default=MISSING, metadata={\"help\": \"path to wav2vec 2.0 model\"}\n",
    "    )\n",
    "    no_pretrained_weights: bool = field(\n",
    "        default=False, metadata={\"help\": \"if true, does not load pretrained weights\"}\n",
    "    )\n",
    "    dropout_input: float = field(\n",
    "        default=0.0,\n",
    "        metadata={\"help\": \"dropout to apply to the input (after feat extr)\"},\n",
    "    )\n",
    "    final_dropout: float = field(\n",
    "        default=0.0,\n",
    "        metadata={\"help\": \"dropout after transformer and before final projection\"},\n",
    "    )\n",
    "    dropout: float = field(\n",
    "        default=0.0, metadata={\"help\": \"dropout probability inside wav2vec 2.0 model\"}\n",
    "    )\n",
    "    attention_dropout: float = field(\n",
    "        default=0.0,\n",
    "        metadata={\n",
    "            \"help\": \"dropout probability for attention weights inside wav2vec 2.0 model\"\n",
    "        },\n",
    "    )\n",
    "    activation_dropout: float = field(\n",
    "        default=0.0,\n",
    "        metadata={\n",
    "            \"help\": \"dropout probability after activation in FFN inside wav2vec 2.0 model\"\n",
    "        },\n",
    "    )\n",
    "    conv_feature_layers: Optional[str] = field(\n",
    "        default=\"[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]\",\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"string describing convolutional feature extraction \"\n",
    "                \"layers in form of a python list that contains \"\n",
    "                \"[(dim, kernel_size, stride), ...]\"\n",
    "            ),\n",
    "        },\n",
    "    )\n",
    "    encoder_embed_dim: Optional[int] = field(\n",
    "        default=768, metadata={\"help\": \"encoder embedding dimension\"}\n",
    "    )\n",
    "\n",
    "    # masking\n",
    "    apply_mask: bool = field(\n",
    "        default=False, metadata={\"help\": \"apply masking during fine-tuning\"}\n",
    "    )\n",
    "    mask_length: int = field(\n",
    "        default=10, metadata={\"help\": \"repeat the mask indices multiple times\"}\n",
    "    )\n",
    "    mask_prob: float = field(\n",
    "        default=0.5,\n",
    "        metadata={\n",
    "            \"help\": \"probability of replacing a token with mask (normalized by length)\"\n",
    "        },\n",
    "    )\n",
    "    mask_selection: MASKING_DISTRIBUTION_CHOICES = field(\n",
    "        default=\"static\", metadata={\"help\": \"how to choose masks\"}\n",
    "    )\n",
    "    mask_other: float = field(\n",
    "        default=0,\n",
    "        metadata={\n",
    "            \"help\": \"secondary mask argument (used for more complex distributions), \"\n",
    "            \"see help in compute_mask_indices\"\n",
    "        },\n",
    "    )\n",
    "    no_mask_overlap: bool = field(\n",
    "        default=False, metadata={\"help\": \"whether to allow masks to overlap\"}\n",
    "    )\n",
    "    mask_min_space: Optional[int] = field(\n",
    "        default=1,\n",
    "        metadata={\"help\": \"min space between spans (if no overlap is enabled)\"},\n",
    "    )\n",
    "\n",
    "    # channel masking\n",
    "    mask_channel_length: int = field(\n",
    "        default=10, metadata={\"help\": \"length of the mask for features (channels)\"}\n",
    "    )\n",
    "    mask_channel_prob: float = field(\n",
    "        default=0.0, metadata={\"help\": \"probability of replacing a feature with 0\"}\n",
    "    )\n",
    "    mask_channel_selection: MASKING_DISTRIBUTION_CHOICES = field(\n",
    "        default=\"static\",\n",
    "        metadata={\"help\": \"how to choose mask length for channel masking\"},\n",
    "    )\n",
    "    mask_channel_other: float = field(\n",
    "        default=0,\n",
    "        metadata={\n",
    "            \"help\": \"secondary mask argument (used for more complex distributions), \"\n",
    "            \"see help in compute_mask_indicesh\"\n",
    "        },\n",
    "    )\n",
    "    no_mask_channel_overlap: bool = field(\n",
    "        default=False, metadata={\"help\": \"whether to allow channel masks to overlap\"}\n",
    "    )\n",
    "    freeze_finetune_updates: int = field(\n",
    "        default=0, metadata={\"help\": \"dont finetune wav2vec for this many updates\"}\n",
    "    )\n",
    "    feature_grad_mult: float = field(\n",
    "        default=0.0, metadata={\"help\": \"reset feature grad mult in wav2vec 2.0 to this\"}\n",
    "    )\n",
    "    layerdrop: float = field(\n",
    "        default=0.0, metadata={\"help\": \"probability of dropping a layer in wav2vec 2.0\"}\n",
    "    )\n",
    "    mask_channel_min_space: Optional[int] = field(\n",
    "        default=1,\n",
    "        metadata={\"help\": \"min space between spans (if no overlap is enabled)\"},\n",
    "    )\n",
    "    mask_channel_before: bool = False\n",
    "    normalize: bool = II(\"task.normalize\")\n",
    "    data: str = II(\"task.data\")\n",
    "    # this holds the loaded wav2vec args\n",
    "    w2v_args: Any = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "68b0ffae-038e-406d-aaca-b46c15c426f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wav2VecEncoder(FairseqEncoder):\n",
    "    def __init__(self, cfg: Wav2Vec2AsrConfig, output_size=None):\n",
    "        self.apply_mask = cfg.apply_mask\n",
    "\n",
    "        arg_overrides = {\n",
    "            \"dropout\": cfg.dropout,\n",
    "            \"activation_dropout\": cfg.activation_dropout,\n",
    "            \"dropout_input\": cfg.dropout_input,\n",
    "            \"attention_dropout\": cfg.attention_dropout,\n",
    "            \"mask_length\": cfg.mask_length,\n",
    "            \"mask_prob\": cfg.mask_prob,\n",
    "            \"mask_selection\": cfg.mask_selection,\n",
    "            \"mask_other\": cfg.mask_other,\n",
    "            \"no_mask_overlap\": cfg.no_mask_overlap,\n",
    "            \"mask_channel_length\": cfg.mask_channel_length,\n",
    "            \"mask_channel_prob\": cfg.mask_channel_prob,\n",
    "            \"mask_channel_before\": cfg.mask_channel_before,\n",
    "            \"mask_channel_selection\": cfg.mask_channel_selection,\n",
    "            \"mask_channel_other\": cfg.mask_channel_other,\n",
    "            \"no_mask_channel_overlap\": cfg.no_mask_channel_overlap,\n",
    "            \"encoder_layerdrop\": cfg.layerdrop,\n",
    "            \"feature_grad_mult\": cfg.feature_grad_mult,\n",
    "        }\n",
    "        \n",
    "        if cfg.w2v_args is None:\n",
    "            state = checkpoint_utils.load_checkpoint_to_cpu(cfg.w2v_path, arg_overrides)\n",
    "            # Get the config of loaed w2v model\n",
    "            w2v_args = state.get(\"cfg\", None)\n",
    "            if w2v_args is None:\n",
    "                w2v_args = convert_namespace_to_omegaconf(state[\"args\"])\n",
    "            w2v_args.criterion = None\n",
    "            w2v_args.lr_scheduler = None\n",
    "            cfg.w2v_args = w2v_args \n",
    "        else:\n",
    "            state = None\n",
    "            w2v_args = cfg.w2v_args\n",
    "            if isinstance(w2v_args, Namespace):\n",
    "                cfg.w2v_args = w2v_args = convert_namespace_to_omegaconf(w2v_args)\n",
    "    \n",
    "        # w2v_args.task -> Config for pre-training\n",
    "        # cfg -> Config for fine-tuning\n",
    "        assert cfg.normalize == w2v_args.task.normalize, (\n",
    "            \"Fine-tuning works best when data normalization is the same. \"\n",
    "            \"Please check that --normalize is set or unset for both pre-training and here\"\n",
    "        )\n",
    "        # Here, data for fine-tuning maybe...\n",
    "        w2v_args.task.data = cfg.data\n",
    "        \n",
    "        # Does not support for loading fine-tuned parameters yet\n",
    "        if w2v_args.model._name == 'wav2vec_ctc':\n",
    "            w2v_config = w2v_args.model.w2v_args.model\n",
    "        elif w2v_args.model._name == 'wav2vec2':\n",
    "            w2v_config = w2v_args.model\n",
    "        else:\n",
    "            w2v_config = None\n",
    "        \n",
    "        w2v_config = convert_to_custom_config(w2v_config)\n",
    "        task = tasks.setup_task(w2v_args.task)\n",
    "        #model = task.build_model(w2v_args.model)\n",
    "        model = Wav2Vec2Model(w2v_config)\n",
    "             \n",
    "        if state is not None and not cfg.no_pretrained_weights:\n",
    "            model.load_state_dict(state[\"model\"], strict=True)\n",
    "\n",
    "        model.remove_pretraining_modules()\n",
    "\n",
    "        super().__init__(task.source_dictionary)\n",
    "\n",
    "        d = w2v_args.model.encoder_embed_dim\n",
    "\n",
    "        self.w2v_model = model\n",
    "\n",
    "        self.final_dropout = nn.Dropout(cfg.final_dropout)\n",
    "        self.freeze_finetune_updates = cfg.freeze_finetune_updates\n",
    "        self.num_updates = 0\n",
    "\n",
    "        targ_d = None\n",
    "        self.proj = None\n",
    "\n",
    "        if output_size is not None:\n",
    "            targ_d = output_size\n",
    "        elif getattr(cfg, \"decoder_embed_dim\", d) != d:\n",
    "            targ_d = cfg.decoder_embed_dim\n",
    "\n",
    "        if targ_d is not None:\n",
    "            self.proj = Linear(d, targ_d)\n",
    "\n",
    "    def set_num_updates(self, num_updates):\n",
    "        \"\"\"Set the number of parameters updates.\"\"\"\n",
    "        super().set_num_updates(num_updates)\n",
    "        self.num_updates = num_updates\n",
    "\n",
    "    def forward(self, source, padding_mask, **kwargs):\n",
    "\n",
    "        w2v_args = {\n",
    "            \"source\": source,\n",
    "            \"padding_mask\": padding_mask,\n",
    "            \"mask\": self.apply_mask and self.training,\n",
    "        }\n",
    "\n",
    "        ft = self.freeze_finetune_updates <= self.num_updates\n",
    "\n",
    "        with torch.no_grad() if not ft else contextlib.ExitStack():\n",
    "            res = self.w2v_model.extract_features(**w2v_args)\n",
    "\n",
    "            x = res[\"x\"]\n",
    "            padding_mask = res[\"padding_mask\"]\n",
    "\n",
    "            # B x T x C -> T x B x C\n",
    "            x = x.transpose(0, 1)\n",
    "\n",
    "        x = self.final_dropout(x)\n",
    "\n",
    "        if self.proj:\n",
    "            x = self.proj(x)\n",
    "\n",
    "        return {\n",
    "            \"encoder_out\": x,  # T x B x C\n",
    "            \"padding_mask\": padding_mask,  # B x T,\n",
    "            \"layer_results\": res[\"layer_results\"],\n",
    "        }\n",
    "\n",
    "    def forward_torchscript(self, net_input):\n",
    "        if torch.jit.is_scripting():\n",
    "            return self.forward(net_input[\"source\"], net_input[\"padding_mask\"])\n",
    "        else:\n",
    "            return self.forward_non_torchscript(net_input)\n",
    "\n",
    "    def reorder_encoder_out(self, encoder_out, new_order):\n",
    "        if encoder_out[\"encoder_out\"] is not None:\n",
    "            encoder_out[\"encoder_out\"] = encoder_out[\"encoder_out\"].index_select(\n",
    "                1, new_order\n",
    "            )\n",
    "        if encoder_out[\"padding_mask\"] is not None:\n",
    "            encoder_out[\"padding_mask\"] = encoder_out[\n",
    "                \"padding_mask\"\n",
    "            ].index_select(0, new_order)\n",
    "        return encoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bf3c788d-4b4b-44c1-8e88-c99014be812b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2AsrConfig(_name=None, w2v_path='/home/kangwook/fairseq/jkw/parameters/libri960_big.pt', no_pretrained_weights=False, dropout_input=0.0, final_dropout=0.0, dropout=0.0, attention_dropout=0.0, activation_dropout=0.0, conv_feature_layers='[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', encoder_embed_dim=768, apply_mask=False, mask_length=10, mask_prob=0.5, mask_selection='static', mask_other=0, no_mask_overlap=False, mask_min_space=1, mask_channel_length=10, mask_channel_prob=0.0, mask_channel_selection='static', mask_channel_other=0, no_mask_channel_overlap=False, freeze_finetune_updates=0, feature_grad_mult=0.0, layerdrop=0.0, mask_channel_min_space=1, mask_channel_before=False, normalize=False, data='${task.data}', w2v_args=None)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Wav2Vec2AsrConfig()\n",
    "\n",
    "# Must get parameters without fine-tuning\n",
    "# or you need your own dictionary\n",
    "config.w2v_path = \"/home/kangwook/fairseq/jkw/parameters/libri960_big.pt\"\n",
    "config.normalize = False\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "47b3f0af-bfc3-4014-af7f-7d4121db4996",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2VecEncoder(\n",
       "  (w2v_model): Wav2Vec2Model(\n",
       "    (feature_extractor): ConvFeatureExtractionModel(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "          (3): GELU()\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): GELU()\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): GELU()\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): GELU()\n",
       "        )\n",
       "        (4): Sequential(\n",
       "          (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): GELU()\n",
       "        )\n",
       "        (5): Sequential(\n",
       "          (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): GELU()\n",
       "        )\n",
       "        (6): Sequential(\n",
       "          (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): GELU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_extract_proj): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (dropout_input): Dropout(p=0.0, inplace=False)\n",
       "    (dropout_features): Dropout(p=0.1, inplace=False)\n",
       "    (quantizer): None\n",
       "    (project_q): None\n",
       "    (encoder): TransformerEncoder(\n",
       "      (pos_conv): Sequential(\n",
       "        (0): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "        (1): SamePad()\n",
       "        (2): GELU()\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerSentenceEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerSentenceEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerSentenceEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerSentenceEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerSentenceEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerSentenceEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): TransformerSentenceEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): TransformerSentenceEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): TransformerSentenceEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): TransformerSentenceEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): TransformerSentenceEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): TransformerSentenceEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (12): TransformerSentenceEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (13): TransformerSentenceEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (14): TransformerSentenceEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (15): TransformerSentenceEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (16): TransformerSentenceEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (17): TransformerSentenceEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (18): TransformerSentenceEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (19): TransformerSentenceEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (20): TransformerSentenceEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (21): TransformerSentenceEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (22): TransformerSentenceEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (23): TransformerSentenceEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (self_attn_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): FusedLayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)\n",
       "    (final_proj): None\n",
       "  )\n",
       "  (final_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (proj): Linear(in_features=1024, out_features=30, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2nd argument is vocabulary size\n",
    "model = Wav2VecEncoder(config, 30)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "14172e95-1544-480f-a63d-3e575094a938",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'children'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-b8283b7c2e91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchinfo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m900\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/py37/lib/python3.7/site-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m     )\n\u001b[1;32m    194\u001b[0m     summary_list = forward_pass(\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_forward_pass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     )\n\u001b[1;32m    197\u001b[0m     \u001b[0mformatting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFormattingOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37/lib/python3.7/site-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0mhooks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRemovableHandle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0mnamed_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0mapply_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamed_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37/lib/python3.7/site-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36mapply_hooks\u001b[0;34m(named_module, orig_model, batch_dim, summary_list, idx, hooks, curr_depth, parent_info)\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         apply_hooks(\n\u001b[0;32m--> 481\u001b[0;31m             \u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m         )\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37/lib/python3.7/site-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36mapply_hooks\u001b[0;34m(named_module, orig_model, batch_dim, summary_list, idx, hooks, curr_depth, parent_info)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# ModuleLists or Sequentials.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0mvar_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnamed_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m     \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLayerInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpre_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37/lib/python3.7/site-packages/torchinfo/layer_info.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, var_name, module, depth, depth_index, parent_info)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_leaf_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'children'"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model.w2v_model, (1, 900))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
