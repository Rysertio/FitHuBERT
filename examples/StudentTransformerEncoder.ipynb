{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4534b3b4-20c2-47bd-b366-fcc4f66db5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Tuple\n",
    "\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from fairseq.dataclass import ChoiceEnum, FairseqDataclass\n",
    "from fairseq.modules import (\n",
    "    Fp32GroupNorm,\n",
    "    Fp32LayerNorm,\n",
    "    GradMultiply,\n",
    "    GumbelVectorQuantizer,\n",
    "    LayerNorm,\n",
    "    MultiheadAttention,\n",
    "    SamePad,\n",
    "    TransposeLast,\n",
    ")\n",
    "\n",
    "from fairseq.modules.transformer_sentence_encoder import init_bert_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e07261f-7c74-402c-8423-fbf72ae815cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TransformerSentenceEncoderLayer import TransformerSentenceEncoderLayer, TransformerSentenceEncoderLayerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68d57f01-7f39-474e-b99a-b4ed715c62dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class StudentTransformerEncoderConfig(FairseqDataclass):\n",
    "    \n",
    "    layer_setting: TransformerSentenceEncoderLayerConfig = field(\n",
    "        default=TransformerSentenceEncoderLayerConfig(),\n",
    "        metadata={\"help\": \"Default setting of TransformerSentenceEncoderLayerConfig\"}\n",
    "    )\n",
    "    \n",
    "    # layer setting after time reduction layer\n",
    "    # You need to change this inside the class\n",
    "    '''\n",
    "    smaller_layer_setting: TransformerSentenceEncoderLayerConfig = field(\n",
    "        default=TransformerSentenceEncoderLayerConfig(\n",
    "            encoder_embed_dim = 384,\n",
    "            encoder_ffn_embed_dim = 1536,\n",
    "            encoder_attention_heads = 6 \n",
    "        ),\n",
    "        metadata={\"help\": \"Time reduction layer of TransformerSentenceEncoderLayerConfig\"}\n",
    "    )\n",
    "    '''\n",
    "    encoder_layers: int = field(\n",
    "        default=6,\n",
    "        metadata={\"help\": \"num encoder layers in the transformer\"}\n",
    "    )\n",
    "    \n",
    "    conv_pos: int = field(\n",
    "        default=128,\n",
    "        metadata={\"help\": \"number of filters for convolutional positional embeddings\"},\n",
    "    )\n",
    "    \n",
    "    conv_pos_groups: int = field(\n",
    "        default=16,\n",
    "        metadata={\"help\": \"number of groups for convolutional positional embedding\"},\n",
    "    )\n",
    "    \n",
    "    encoder_layerdrop: float = field(\n",
    "        default=0.0,\n",
    "        metadata={\"help\": \"probability of dropping a transformer layer\"}\n",
    "    )\n",
    "    \n",
    "    # Time-reduction layer\n",
    "    able_tr_layer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"applying time reduction layer or not\"}\n",
    "    )\n",
    "    \n",
    "    type_of_tr_layer: str = field(\n",
    "        default=\"fcl\", # or conv1d\n",
    "        metadata={\"help\": \"type of time reduction layer\"}\n",
    "    )\n",
    "    \n",
    "    tr_conv1d_kernel_stride: str = field(\n",
    "        default=\"(2, 2)\",\n",
    "        metadata={\"help\": \"If tr is conv1d, list of kernel and stride for conv1d\"}\n",
    "    )\n",
    "    \n",
    "    tr_fcl_output_factor: int = field(\n",
    "        default=2,\n",
    "        metadata={\"help\": \"Factor to reduce time length\"}\n",
    "    )\n",
    "    \n",
    "    tr_layer_floor: int = field(\n",
    "        default=3,\n",
    "        metadata={\"help\": \"which floor should time reduction layer put in\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a84700e3-a7a7-421a-b521-529f26f7d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentTransformerEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                cfg: StudentTransformerEncoderConfig\n",
    "                ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        args = cfg.layer_setting\n",
    "        \n",
    "        self.dropout = args.dropout\n",
    "        self.embedding_dim = args.encoder_embed_dim\n",
    "\n",
    "        self.pos_conv = nn.Conv1d(\n",
    "            self.embedding_dim,\n",
    "            self.embedding_dim,\n",
    "            kernel_size=cfg.conv_pos,\n",
    "            padding=cfg.conv_pos // 2,\n",
    "            groups=cfg.conv_pos_groups,\n",
    "        )\n",
    "        dropout = 0\n",
    "        std = math.sqrt((4 * (1.0 - dropout)) / (cfg.conv_pos * self.embedding_dim))\n",
    "        nn.init.normal_(self.pos_conv.weight, mean=0, std=std)\n",
    "        nn.init.constant_(self.pos_conv.bias, 0)    \n",
    "        \n",
    "        self.pos_conv = nn.utils.weight_norm(self.pos_conv, name=\"weight\", dim=2)\n",
    "        self.pos_conv = nn.Sequential(self.pos_conv, SamePad(cfg.conv_pos), nn.GELU())\n",
    "\n",
    "        self.tr_fcl_output_factor = None\n",
    "        if not cfg.able_tr_layer:\n",
    "            tr_layer = None  \n",
    "        else:\n",
    "            if cfg.type_of_tr_layer == 'fcl':\n",
    "                self.tr_fcl_output_factor = cfg.tr_fcl_output_factor\n",
    "                # Input length will be verified first.\n",
    "                tr_layer = nn.Linear(\n",
    "                    self.embedding_dim * self.tr_fcl_output_factor,\n",
    "                    self.embedding_dim\n",
    "                )\n",
    "                nn.init.xavier_uniform_(tr_layer.weight)\n",
    "                \n",
    "            elif cfg.type_of_tr_layer == 'conv1d':\n",
    "                (kernel, stride) = eval(cfg.tr_conv1d_kernel_stride)\n",
    "                tr_layer = nn.Conv1d(\n",
    "                    self.embedding_dim,\n",
    "                    self.embedding_dim,\n",
    "                    kernel_size=kernel,\n",
    "                    stride=stride\n",
    "                )\n",
    "            else:\n",
    "                print (\"Wrong type of time reduction layer.\")             \n",
    "        self.tr_layer = tr_layer\n",
    "        \n",
    "        if not cfg.able_tr_layer:\n",
    "            self.layers = nn.ModuleList(\n",
    "                [\n",
    "                    TransformerSentenceEncoderLayer(cfg.layer_setting)\n",
    "                    for _ in range(cfg.encoder_layers)\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            # To do: # get list of {tr_layer, and where to put in}\n",
    "            # And put in literally using nn.module list\n",
    "            '''\n",
    "            self.layers = nn.ModuleList(\n",
    "                    [\n",
    "                        TransformerSentenceEncoderLayer(cfg.layer_setting)\n",
    "                        for _ in range(cfg.encoder_layers)\n",
    "                    ].insert(cfg.tr_layer_floor, self.tr_layer)\n",
    "            )\n",
    "            \n",
    "            '''\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        TransformerSentenceEncoderLayer(cfg.layer_setting)\n",
    "                        for _ in range(cfg.tr_layer_floor)\n",
    "                    ],\n",
    "                ),\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        self.tr_layer\n",
    "                    ]\n",
    "                     ),\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        TransformerSentenceEncoderLayer(cfg.layer_setting)\n",
    "                        for _ in range(cfg.encoder_layers - cfg.tr_layer_floor)\n",
    "                    ],                                \n",
    "                )\n",
    "            \n",
    "            )\n",
    "\n",
    "        self.layer_norm_first = args.layer_norm_first\n",
    "        self.layer_norm = LayerNorm(self.embedding_dim)\n",
    "        self.layerdrop = cfg.encoder_layerdrop\n",
    "\n",
    "        self.apply(init_bert_params)\n",
    "\n",
    "    def forward(self, x, padding_mask=None, layer=None):\n",
    "        x, layer_results = self.extract_features(x, padding_mask, layer)\n",
    "\n",
    "        if self.layer_norm_first and layer is None:\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "        return x, layer_results\n",
    "\n",
    "    def extract_features(self, x, padding_mask=None, tgt_layer=None):\n",
    "\n",
    "        if padding_mask is not None:\n",
    "            x = index_put(x, padding_mask, 0)\n",
    "\n",
    "        x_conv = self.pos_conv(x.transpose(1, 2))\n",
    "        x_conv = x_conv.transpose(1, 2)\n",
    "        x = x + x_conv\n",
    "\n",
    "        if not self.layer_norm_first:\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # B x T x C -> T x B x C\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        layer_results = []\n",
    "        r = None\n",
    "        \n",
    "        if self.tr_layer is None:\n",
    "            for j, layer in enumerate(self.layers):\n",
    "                dropout_probability = np.random.random()\n",
    "                if not self.training or (dropout_probability > self.layerdrop):\n",
    "                    x, z = layer(x, self_attn_padding_mask=padding_mask, need_weights=False)\n",
    "                    if tgt_layer is not None:\n",
    "                        layer_results.append((x, z))\n",
    "                if j == tgt_layer:\n",
    "                    r = x\n",
    "                    break                \n",
    "        else:\n",
    "            # To do\n",
    "            \"\"\"\n",
    "            for j, layer in enumerate(self.layers):\n",
    "                if isinstance(layer, TransformerEncoderLayer):\n",
    "                    dropout_probability = np.random.random()\n",
    "                    if not self.training or (dropout_probability > self.layerdrop):\n",
    "                        x, z = layer(x, self_attn_padding_mask=padding_mask, need_weights=False)\n",
    "                        if tgt_layer is not None:\n",
    "                            layer_results.append((x, z))\n",
    "                    if i == tgt_layer:\n",
    "                        r = x\n",
    "                        break\n",
    "                elif isinstance(layer, torch.nn.Conv1d): \n",
    "                    x = x.permute(1, 2, 0).contiguous()\n",
    "                    x = layer(x)\n",
    "                    x = x.permute(2, 0, 1).contiguous()\n",
    "                elif isinstance(layer, torch.nn.Linear):\n",
    "                    # T x B x C\n",
    "                    x = self.concat_channelwise(x)\n",
    "                    x = layer(x) \n",
    "            \"\"\"\n",
    "            \n",
    "            for i, layer_block in enumerate(self.layers):\n",
    "                # I write this code in this way intentionally\n",
    "                # TransformerEnocder             \n",
    "                if i == 0:\n",
    "                    for j, layer in enumerate(layer_block):\n",
    "                        dropout_probability = np.random.random()\n",
    "                        if not self.training or (dropout_probability > self.layerdrop):\n",
    "                            x, z = layer(x, self_attn_padding_mask=padding_mask, need_weights=False)\n",
    "                            if tgt_layer is not None:\n",
    "                                layer_results.append((x, z))\n",
    "                        if i == tgt_layer:\n",
    "                            r = x\n",
    "                            break\n",
    "                # Time Reduction\n",
    "                elif i == 1:   \n",
    "                    for j, layer in enumerate(layer_block):\n",
    "                        if isinstance(layer, torch.nn.Conv1d): \n",
    "                            x = x.permute(1, 2, 0).contiguous()\n",
    "                            x = layer(x)\n",
    "                            x = x.permute(2, 0, 1).contiguous()\n",
    "                        elif isinstance(layer, torch.nn.Linear):\n",
    "                            # T x B x C\n",
    "                            x = self.concat_channelwise(x)\n",
    "                            x = layer(x)\n",
    "                # TransformerEncoder\n",
    "                elif i == 2:\n",
    "                    for j, layer in enumerate(layer_block):\n",
    "                        dropout_probability = np.random.random()\n",
    "                        if not self.training or (dropout_probability > self.layerdrop):\n",
    "                            x, z = layer(x, self_attn_padding_mask=padding_mask, need_weights=False)\n",
    "                            if tgt_layer is not None:\n",
    "                                layer_results.append((x, z))\n",
    "                        if i == tgt_layer:\n",
    "                            r = x\n",
    "                            break\n",
    "        \n",
    "        if r is not None:\n",
    "            x = r\n",
    "\n",
    "        # T x B x C -> B x T x C\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        return x, layer_results    \n",
    "    \n",
    "    def max_positions(self):\n",
    "        \"\"\"Maximum output length supported by the encoder.\"\"\"\n",
    "        return self.args.max_positions\n",
    "\n",
    "    def upgrade_state_dict_named(self, state_dict, name):\n",
    "        \"\"\"Upgrade a (possibly old) state dic\\t for new versions of fairseq.\"\"\"\n",
    "        return state_dict\n",
    "    \n",
    "    def concat_channelwise(self, x):\n",
    "        # x is shaped T x B x C\n",
    "        time_length, batch, channel = x.size()\n",
    "        how_many_pad = self.tr_fcl_output_factor - time_length % self.tr_fcl_output_factor \n",
    "        if how_many_pad != 0:\n",
    "            zero_pad = torch.zeros([how_many_pad, batch, channel]).cuda()\n",
    "            x = torch.cat([x, zero_pad], dim = 0)\n",
    "        time_length += how_many_pad\n",
    "\n",
    "        result = torch.tensor([]).cuda()\n",
    "        \n",
    "        j = 0\n",
    "        while (j < self.tr_fcl_output_factor):\n",
    "            # (T / factor) X B x C\n",
    "            tensor_to_concat = x[j::self.tr_fcl_output_factor,:,:]\n",
    "            result = torch.cat([result, tensor_to_concat], dim = 2)\n",
    "            j += 1\n",
    "        # (T / factor) X B X (C * factor)\n",
    "        return result         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9e18bedb-116a-48a0-926a-89319b0dc1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StudentTransformerEncoderConfig(_name=None, layer_setting=TransformerSentenceEncoderLayerConfig(_name=None, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_attention_heads=12, dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, activation_fn='gelu', layer_norm_first=False), encoder_layers=6, conv_pos=128, conv_pos_groups=16, encoder_layerdrop=0.0, able_tr_layer=True, type_of_tr_layer='fcl', tr_conv1d_kernel_stride='(2, 2)', tr_fcl_output_factor=2, tr_layer_floor=3)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = StudentTransformerEncoderConfig(\n",
    "    #/able_tr_layer = False,\n",
    "    type_of_tr_layer = \"fcl\",\n",
    "    tr_fcl_output_factor = 2\n",
    "    )\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f742943e-f22f-49b0-9097-cbf042aea495",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StudentTransformerEncoder(\n",
       "  (pos_conv): Sequential(\n",
       "    (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "    (1): SamePad()\n",
       "    (2): GELU()\n",
       "  )\n",
       "  (tr_layer): Linear(in_features=1536, out_features=768, bias=True)\n",
       "  (layers): ModuleList()\n",
       "  (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = StudentTransformerEncoder(config)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3a4c5b4d-97da-44ce-a573-b52256e93ae8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n",
       "===================================================================================================================\n",
       "StudentTransformerEncoder                --                        --                        --\n",
       "├─ModuleList: 1-1                        --                        --                        --\n",
       "├─Sequential: 1-2                        [1, 768, 333]             [1, 768, 333]             --\n",
       "│    └─Conv1d: 2-1                       [1, 768, 333]             [1, 768, 334]             4,719,488\n",
       "│    └─SamePad: 2-2                      [1, 768, 334]             [1, 768, 333]             --\n",
       "│    └─GELU: 2-3                         [1, 768, 333]             [1, 768, 333]             --\n",
       "├─FusedLayerNorm: 1-3                    [1, 333, 768]             [1, 333, 768]             1,536\n",
       "===================================================================================================================\n",
       "Total params: 4,721,024\n",
       "Trainable params: 4,721,024\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 3.62\n",
       "===================================================================================================================\n",
       "Input size (MB): 1.02\n",
       "Forward/backward pass size (MB): 4.10\n",
       "Params size (MB): 18.88\n",
       "Estimated Total Size (MB): 24.01\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, (1, 333, 768), col_names = [\"input_size\", \"output_size\", \"num_params\"], depth = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c8a9c8-e2c9-48bf-bce4-f811d05370d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
