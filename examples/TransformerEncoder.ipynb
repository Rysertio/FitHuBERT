{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4534b3b4-20c2-47bd-b366-fcc4f66db5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Tuple\n",
    "\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from fairseq.dataclass import ChoiceEnum, FairseqDataclass\n",
    "from fairseq.modules import (\n",
    "    Fp32GroupNorm,\n",
    "    Fp32LayerNorm,\n",
    "    GradMultiply,\n",
    "    GumbelVectorQuantizer,\n",
    "    LayerNorm,\n",
    "    MultiheadAttention,\n",
    "    SamePad,\n",
    "    TransposeLast,\n",
    ")\n",
    "\n",
    "from fairseq.modules.transformer_sentence_encoder import init_bert_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e07261f-7c74-402c-8423-fbf72ae815cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TransformerSentenceEncoderLayer import TransformerSentenceEncoderLayer, TransformerSentenceEncoderLayerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68d57f01-7f39-474e-b99a-b4ed715c62dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerEncoderConfig(FairseqDataclass):\n",
    "    \n",
    "    layer_setting: TransformerSentenceEncoderLayerConfig = field(\n",
    "        default=TransformerSentenceEncoderLayerConfig(),\n",
    "        metadata={\"help\": \"Default setting of TransformerSentenceEncoderLayerConfig\"}\n",
    "    )\n",
    "    \n",
    "    encoder_layers: int = field(\n",
    "        default=12,\n",
    "        metadata={\"help\": \"num encoder layers in the transformer\"}\n",
    "    )\n",
    "    \n",
    "    conv_pos: int = field(\n",
    "        default=128,\n",
    "        metadata={\"help\": \"number of filters for convolutional positional embeddings\"},\n",
    "    )\n",
    "    \n",
    "    conv_pos_groups: int = field(\n",
    "        default=16,\n",
    "        metadata={\"help\": \"number of groups for convolutional positional embedding\"},\n",
    "    )\n",
    "    \n",
    "    encoder_layerdrop: float = field(\n",
    "        default=0.0,\n",
    "        metadata={\"help\": \"probability of dropping a transformer layer\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a84700e3-a7a7-421a-b521-529f26f7d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                cfg: TransformerEncoderConfig\n",
    "                ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        args = cfg.layer_setting\n",
    "        \n",
    "        self.dropout = args.dropout\n",
    "        self.embedding_dim = args.encoder_embed_dim\n",
    "\n",
    "        self.pos_conv = nn.Conv1d(\n",
    "            self.embedding_dim,\n",
    "            self.embedding_dim,\n",
    "            kernel_size=cfg.conv_pos,\n",
    "            padding=cfg.conv_pos // 2,\n",
    "            groups=cfg.conv_pos_groups,\n",
    "        )\n",
    "        dropout = 0\n",
    "        std = math.sqrt((4 * (1.0 - dropout)) / (cfg.conv_pos * self.embedding_dim))\n",
    "        nn.init.normal_(self.pos_conv.weight, mean=0, std=std)\n",
    "        nn.init.constant_(self.pos_conv.bias, 0)\n",
    "\n",
    "        self.pos_conv = nn.utils.weight_norm(self.pos_conv, name=\"weight\", dim=2)\n",
    "        self.pos_conv = nn.Sequential(self.pos_conv, SamePad(cfg.conv_pos), nn.GELU())\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerSentenceEncoderLayer(\n",
    "                    cfg.layer_setting\n",
    "                )\n",
    "                for _ in range(cfg.encoder_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.layer_norm_first = args.layer_norm_first\n",
    "        self.layer_norm = LayerNorm(self.embedding_dim)\n",
    "        self.layerdrop = cfg.encoder_layerdrop\n",
    "\n",
    "        self.apply(init_bert_params)\n",
    "\n",
    "    def forward(self, x, padding_mask=None, layer=None):\n",
    "        x, layer_results = self.extract_features(x, padding_mask, layer)\n",
    "\n",
    "        if self.layer_norm_first and layer is None:\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "        return x, layer_results\n",
    "\n",
    "    def extract_features(self, x, padding_mask=None, tgt_layer=None):\n",
    "\n",
    "        if padding_mask is not None:\n",
    "            x = index_put(x, padding_mask, 0)\n",
    "\n",
    "        x_conv = self.pos_conv(x.transpose(1, 2))\n",
    "        x_conv = x_conv.transpose(1, 2)\n",
    "        x = x + x_conv\n",
    "\n",
    "        if not self.layer_norm_first:\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # B x T x C -> T x B x C\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        layer_results = []\n",
    "        r = None\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            dropout_probability = np.random.random()\n",
    "            if not self.training or (dropout_probability > self.layerdrop):\n",
    "                x, z = layer(x, self_attn_padding_mask=padding_mask, need_weights=False)\n",
    "                if tgt_layer is not None:\n",
    "                    layer_results.append((x, z))\n",
    "            if i == tgt_layer:\n",
    "                r = x\n",
    "                break\n",
    "\n",
    "        if r is not None:\n",
    "            x = r\n",
    "\n",
    "        # T x B x C -> B x T x C\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        return x, layer_results\n",
    "\n",
    "    def max_positions(self):\n",
    "        \"\"\"Maximum output length supported by the encoder.\"\"\"\n",
    "        return self.args.max_positions\n",
    "\n",
    "    def upgrade_state_dict_named(self, state_dict, name):\n",
    "        \"\"\"Upgrade a (possibly old) state dict for new versions of fairseq.\"\"\"\n",
    "        return state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9e18bedb-116a-48a0-926a-89319b0dc1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoderConfig(_name=None, layer_setting=TransformerSentenceEncoderLayerConfig(_name=None, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_attention_heads=12, dropout=0.1, attention_dropout=0.1, activation_dropout=0.0, activation_fn='gelu', layer_norm_first=False), encoder_layers=12, conv_pos=128, conv_pos_groups=16, encoder_layerdrop=0.0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = TransformerEncoderConfig()\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f742943e-f22f-49b0-9097-cbf042aea495",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (pos_conv): Sequential(\n",
       "    (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "    (1): SamePad()\n",
       "    (2): GELU()\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0): TransformerSentenceEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): TransformerSentenceEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): TransformerSentenceEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): TransformerSentenceEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): TransformerSentenceEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): TransformerSentenceEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (6): TransformerSentenceEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (7): TransformerSentenceEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (8): TransformerSentenceEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (9): TransformerSentenceEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (10): TransformerSentenceEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (11): TransformerSentenceEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerEncoder(config)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3a4c5b4d-97da-44ce-a573-b52256e93ae8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "TransformerEncoder                                 --                        --\n",
       "├─ModuleList: 1-1                                  --                        --\n",
       "├─Sequential: 1-2                                  [1, 768, 333]             --\n",
       "│    └─Conv1d: 2-1                                 [1, 768, 334]             4,719,488\n",
       "│    └─SamePad: 2-2                                [1, 768, 333]             --\n",
       "│    └─GELU: 2-3                                   [1, 768, 333]             --\n",
       "├─FusedLayerNorm: 1-3                              [1, 333, 768]             1,536\n",
       "├─ModuleList: 1-1                                  --                        --\n",
       "│    └─TransformerSentenceEncoderLayer: 2-4        [333, 1, 768]             --\n",
       "│    │    └─MultiheadAttention: 3-1                [333, 1, 768]             2,362,368\n",
       "│    │    └─Dropout: 3-2                           [333, 1, 768]             --\n",
       "│    │    └─FusedLayerNorm: 3-3                    [333, 1, 768]             1,536\n",
       "│    │    └─Linear: 3-4                            [333, 1, 3072]            2,362,368\n",
       "│    │    └─Dropout: 3-5                           [333, 1, 3072]            --\n",
       "│    │    └─Linear: 3-6                            [333, 1, 768]             2,360,064\n",
       "│    │    └─Dropout: 3-7                           [333, 1, 768]             --\n",
       "│    │    └─FusedLayerNorm: 3-8                    [333, 1, 768]             1,536\n",
       "│    └─TransformerSentenceEncoderLayer: 2-5        [333, 1, 768]             --\n",
       "│    │    └─MultiheadAttention: 3-9                [333, 1, 768]             2,362,368\n",
       "│    │    └─Dropout: 3-10                          [333, 1, 768]             --\n",
       "│    │    └─FusedLayerNorm: 3-11                   [333, 1, 768]             1,536\n",
       "│    │    └─Linear: 3-12                           [333, 1, 3072]            2,362,368\n",
       "│    │    └─Dropout: 3-13                          [333, 1, 3072]            --\n",
       "│    │    └─Linear: 3-14                           [333, 1, 768]             2,360,064\n",
       "│    │    └─Dropout: 3-15                          [333, 1, 768]             --\n",
       "│    │    └─FusedLayerNorm: 3-16                   [333, 1, 768]             1,536\n",
       "│    └─TransformerSentenceEncoderLayer: 2-6        [333, 1, 768]             --\n",
       "│    │    └─MultiheadAttention: 3-17               [333, 1, 768]             2,362,368\n",
       "│    │    └─Dropout: 3-18                          [333, 1, 768]             --\n",
       "│    │    └─FusedLayerNorm: 3-19                   [333, 1, 768]             1,536\n",
       "│    │    └─Linear: 3-20                           [333, 1, 3072]            2,362,368\n",
       "│    │    └─Dropout: 3-21                          [333, 1, 3072]            --\n",
       "│    │    └─Linear: 3-22                           [333, 1, 768]             2,360,064\n",
       "│    │    └─Dropout: 3-23                          [333, 1, 768]             --\n",
       "│    │    └─FusedLayerNorm: 3-24                   [333, 1, 768]             1,536\n",
       "│    └─TransformerSentenceEncoderLayer: 2-7        [333, 1, 768]             --\n",
       "│    │    └─MultiheadAttention: 3-25               [333, 1, 768]             2,362,368\n",
       "│    │    └─Dropout: 3-26                          [333, 1, 768]             --\n",
       "│    │    └─FusedLayerNorm: 3-27                   [333, 1, 768]             1,536\n",
       "│    │    └─Linear: 3-28                           [333, 1, 3072]            2,362,368\n",
       "│    │    └─Dropout: 3-29                          [333, 1, 3072]            --\n",
       "│    │    └─Linear: 3-30                           [333, 1, 768]             2,360,064\n",
       "│    │    └─Dropout: 3-31                          [333, 1, 768]             --\n",
       "│    │    └─FusedLayerNorm: 3-32                   [333, 1, 768]             1,536\n",
       "│    └─TransformerSentenceEncoderLayer: 2-8        [333, 1, 768]             --\n",
       "│    │    └─MultiheadAttention: 3-33               [333, 1, 768]             2,362,368\n",
       "│    │    └─Dropout: 3-34                          [333, 1, 768]             --\n",
       "│    │    └─FusedLayerNorm: 3-35                   [333, 1, 768]             1,536\n",
       "│    │    └─Linear: 3-36                           [333, 1, 3072]            2,362,368\n",
       "│    │    └─Dropout: 3-37                          [333, 1, 3072]            --\n",
       "│    │    └─Linear: 3-38                           [333, 1, 768]             2,360,064\n",
       "│    │    └─Dropout: 3-39                          [333, 1, 768]             --\n",
       "│    │    └─FusedLayerNorm: 3-40                   [333, 1, 768]             1,536\n",
       "│    └─TransformerSentenceEncoderLayer: 2-9        [333, 1, 768]             --\n",
       "│    │    └─MultiheadAttention: 3-41               [333, 1, 768]             2,362,368\n",
       "│    │    └─Dropout: 3-42                          [333, 1, 768]             --\n",
       "│    │    └─FusedLayerNorm: 3-43                   [333, 1, 768]             1,536\n",
       "│    │    └─Linear: 3-44                           [333, 1, 3072]            2,362,368\n",
       "│    │    └─Dropout: 3-45                          [333, 1, 3072]            --\n",
       "│    │    └─Linear: 3-46                           [333, 1, 768]             2,360,064\n",
       "│    │    └─Dropout: 3-47                          [333, 1, 768]             --\n",
       "│    │    └─FusedLayerNorm: 3-48                   [333, 1, 768]             1,536\n",
       "│    └─TransformerSentenceEncoderLayer: 2-10       [333, 1, 768]             --\n",
       "│    │    └─MultiheadAttention: 3-49               [333, 1, 768]             2,362,368\n",
       "│    │    └─Dropout: 3-50                          [333, 1, 768]             --\n",
       "│    │    └─FusedLayerNorm: 3-51                   [333, 1, 768]             1,536\n",
       "│    │    └─Linear: 3-52                           [333, 1, 3072]            2,362,368\n",
       "│    │    └─Dropout: 3-53                          [333, 1, 3072]            --\n",
       "│    │    └─Linear: 3-54                           [333, 1, 768]             2,360,064\n",
       "│    │    └─Dropout: 3-55                          [333, 1, 768]             --\n",
       "│    │    └─FusedLayerNorm: 3-56                   [333, 1, 768]             1,536\n",
       "│    └─TransformerSentenceEncoderLayer: 2-11       [333, 1, 768]             --\n",
       "│    │    └─MultiheadAttention: 3-57               [333, 1, 768]             2,362,368\n",
       "│    │    └─Dropout: 3-58                          [333, 1, 768]             --\n",
       "│    │    └─FusedLayerNorm: 3-59                   [333, 1, 768]             1,536\n",
       "│    │    └─Linear: 3-60                           [333, 1, 3072]            2,362,368\n",
       "│    │    └─Dropout: 3-61                          [333, 1, 3072]            --\n",
       "│    │    └─Linear: 3-62                           [333, 1, 768]             2,360,064\n",
       "│    │    └─Dropout: 3-63                          [333, 1, 768]             --\n",
       "│    │    └─FusedLayerNorm: 3-64                   [333, 1, 768]             1,536\n",
       "│    └─TransformerSentenceEncoderLayer: 2-12       [333, 1, 768]             --\n",
       "│    │    └─MultiheadAttention: 3-65               [333, 1, 768]             2,362,368\n",
       "│    │    └─Dropout: 3-66                          [333, 1, 768]             --\n",
       "│    │    └─FusedLayerNorm: 3-67                   [333, 1, 768]             1,536\n",
       "│    │    └─Linear: 3-68                           [333, 1, 3072]            2,362,368\n",
       "│    │    └─Dropout: 3-69                          [333, 1, 3072]            --\n",
       "│    │    └─Linear: 3-70                           [333, 1, 768]             2,360,064\n",
       "│    │    └─Dropout: 3-71                          [333, 1, 768]             --\n",
       "│    │    └─FusedLayerNorm: 3-72                   [333, 1, 768]             1,536\n",
       "│    └─TransformerSentenceEncoderLayer: 2-13       [333, 1, 768]             --\n",
       "│    │    └─MultiheadAttention: 3-73               [333, 1, 768]             2,362,368\n",
       "│    │    └─Dropout: 3-74                          [333, 1, 768]             --\n",
       "│    │    └─FusedLayerNorm: 3-75                   [333, 1, 768]             1,536\n",
       "│    │    └─Linear: 3-76                           [333, 1, 3072]            2,362,368\n",
       "│    │    └─Dropout: 3-77                          [333, 1, 3072]            --\n",
       "│    │    └─Linear: 3-78                           [333, 1, 768]             2,360,064\n",
       "│    │    └─Dropout: 3-79                          [333, 1, 768]             --\n",
       "│    │    └─FusedLayerNorm: 3-80                   [333, 1, 768]             1,536\n",
       "│    └─TransformerSentenceEncoderLayer: 2-14       [333, 1, 768]             --\n",
       "│    │    └─MultiheadAttention: 3-81               [333, 1, 768]             2,362,368\n",
       "│    │    └─Dropout: 3-82                          [333, 1, 768]             --\n",
       "│    │    └─FusedLayerNorm: 3-83                   [333, 1, 768]             1,536\n",
       "│    │    └─Linear: 3-84                           [333, 1, 3072]            2,362,368\n",
       "│    │    └─Dropout: 3-85                          [333, 1, 3072]            --\n",
       "│    │    └─Linear: 3-86                           [333, 1, 768]             2,360,064\n",
       "│    │    └─Dropout: 3-87                          [333, 1, 768]             --\n",
       "│    │    └─FusedLayerNorm: 3-88                   [333, 1, 768]             1,536\n",
       "│    └─TransformerSentenceEncoderLayer: 2-15       [333, 1, 768]             --\n",
       "│    │    └─MultiheadAttention: 3-89               [333, 1, 768]             2,362,368\n",
       "│    │    └─Dropout: 3-90                          [333, 1, 768]             --\n",
       "│    │    └─FusedLayerNorm: 3-91                   [333, 1, 768]             1,536\n",
       "│    │    └─Linear: 3-92                           [333, 1, 3072]            2,362,368\n",
       "│    │    └─Dropout: 3-93                          [333, 1, 3072]            --\n",
       "│    │    └─Linear: 3-94                           [333, 1, 768]             2,360,064\n",
       "│    │    └─Dropout: 3-95                          [333, 1, 768]             --\n",
       "│    │    └─FusedLayerNorm: 3-96                   [333, 1, 768]             1,536\n",
       "====================================================================================================\n",
       "Total params: 61,427,072\n",
       "Trainable params: 61,427,072\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 22.51\n",
       "====================================================================================================\n",
       "Input size (MB): 1.02\n",
       "Forward/backward pass size (MB): 175.96\n",
       "Params size (MB): 245.71\n",
       "Estimated Total Size (MB): 422.69\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, (1, 333, 768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b38797b-5eff-4c43-950b-eb85eb7c55ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
