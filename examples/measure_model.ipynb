{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed696d90-773a-4546-bd0c-59185190c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_fsq_model import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa023ade-c2de-4793-90c5-b59971608583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'fairseq.tasks.audio_pretraining.AudioPretrainingTask'>\n"
     ]
    }
   ],
   "source": [
    "model = load_model('../wav2vec_small_960h.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eec90b48-f223-406f-b89a-eeab767e1b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.w2v_encoder.w2v_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc004b4d-127b-4dca-9077-009bd18fc95c",
   "metadata": {},
   "source": [
    "## Examine number of parameters(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceb50899-f56c-4c68-8ef8-1c5bf95cfc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94371712 (Wav2Vec2Model)\n",
      "89775488 (TransformerEncoder)\n",
      "4200448 (ConvFeatureExtractionModel)\n",
      "393984 (Linear)\n",
      "1024 (FusedLayerNorm)\n"
     ]
    }
   ],
   "source": [
    "# Examine # of parameters in model\n",
    "def nparams(model):\n",
    "    print(f\"{sum(p.numel() for p in model.parameters())} ({type(model).__name__})\")\n",
    "\n",
    "nparams(model)\n",
    "nparams(model.encoder)\n",
    "nparams(model.feature_extractor)\n",
    "nparams(model.post_extract_proj)\n",
    "nparams(model.layer_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6469bac3-43fa-49d0-89d0-f27b0f67a026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4719488 (Sequential)\n",
      "7087872 (TransformerSentenceEncoderLayer)\n"
     ]
    }
   ],
   "source": [
    "nparams(model.encoder.pos_conv)\n",
    "nparams(model.encoder.layers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ed5588-6db6-4da3-8b41-440e8c34344f",
   "metadata": {},
   "source": [
    "## Examine inference speed(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad4f787b-f2e2-412c-b6ef-be6e184c2622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 166960])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get one audio sample from librispeech\n",
    "import torchaudio\n",
    "test_data = torchaudio.datasets.LIBRISPEECH(\"../\", \"test-clean\", download=True)\n",
    "sample = test_data[0][0]\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f3ead4c-0732-4c36-ae5a-8732ca2cd8b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934ad18181a64f1e862f64c09913b61a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 1: 0.2713434028625488\n",
      "Checkpoint 2: 0.0017110848426818849\n",
      "Checkpoint 3: 0.026907129287719725\n",
      "Checkpoint 4: 0.07916448831558227\n",
      "Checkpoint 5: 0.47131627321243286\n",
      "Whole inference time: 0.7712778902053833\n"
     ]
    }
   ],
   "source": [
    "# Compare inference time of each component\n",
    "import time\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sample = test_data[0][0]\n",
    "\n",
    "ckpt1 = []\n",
    "ckpt2 = []\n",
    "ckpt3 = []\n",
    "ckpt4 = []\n",
    "ckpt5 = []\n",
    "whole = []\n",
    "\n",
    "pbar = tqdm(range(100))\n",
    "\n",
    "for i in pbar:\n",
    "    start_time = time.time()\n",
    "    #conv feature extractor\n",
    "    features = model.feature_extractor(sample)\n",
    "    features = features.transpose(1, 2)\n",
    "    features = model.layer_norm(features)\n",
    "    ckpt1_time = time.time()\n",
    "    ckpt1.append(ckpt1_time - start_time)\n",
    "\n",
    "    #post extract proj\n",
    "    features = model.post_extract_proj(features)\n",
    "    ckpt2_time = time.time()\n",
    "    ckpt2.append(ckpt2_time - ckpt1_time)\n",
    "\n",
    "    #conv position embedding\n",
    "    x_conv = model.encoder.pos_conv(features.transpose(1, 2))\n",
    "    x_conv = x_conv.transpose(1, 2)\n",
    "    x = features + x_conv\n",
    "    x = model.encoder.layer_norm(x)\n",
    "    x = F.dropout(x, p=0.1, training=True)\n",
    "    x = x.transpose(0, 1)\n",
    "    ckpt3_time = time.time()\n",
    "    ckpt3.append(ckpt3_time - ckpt2_time)\n",
    "\n",
    "    #Transformer Layers\n",
    "    for i, layer in enumerate(model.encoder.layers):\n",
    "        x, z = layer(x)\n",
    "        if i==1:\n",
    "            ckpt4_time = time.time()\n",
    "            ckpt4.append(ckpt4_time - ckpt3_time)\n",
    "    x = x.transpose(0, 1)\n",
    "    ckpt5_time = time.time()\n",
    "    ckpt5.append(ckpt5_time - ckpt3_time)\n",
    "    whole.append(ckpt5_time - start_time)\n",
    "    \n",
    "print(f\"Checkpoint 1: {np.mean(ckpt1)}\")\n",
    "print(f\"Checkpoint 2: {np.mean(ckpt2)}\")\n",
    "print(f\"Checkpoint 3: {np.mean(ckpt3)}\")\n",
    "print(f\"Checkpoint 4: {np.mean(ckpt4)}\")\n",
    "print(f\"Checkpoint 5: {np.mean(ckpt5)}\")\n",
    "print(f\"Whole inference time: {np.mean(whole)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7303b7-f6fe-4354-aef6-cf9432d2d873",
   "metadata": {},
   "source": [
    "# Wav2Vec2 Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfeeb3bd-020c-4fd6-9d38-e04e4b3bbede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'fairseq.tasks.audio_pretraining.AudioPretrainingTask'>\n"
     ]
    }
   ],
   "source": [
    "model = load_model('../wav2vec_big_960h.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0250cb36-7f3c-4070-89c5-9c5a189fe569",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.w2v_encoder.w2v_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accefdc9-8065-4f0e-98a2-bcde83f81c87",
   "metadata": {},
   "source": [
    "## Examine number of parameters(large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f261bb9-f7d4-4545-b417-cf86a7ae4851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315428992 (Wav2Vec2Model)\n",
      "310701184 (TransformerEncoder)\n",
      "4200448 (ConvFeatureExtractionModel)\n",
      "525312 (Linear)\n",
      "1024 (FusedLayerNorm)\n"
     ]
    }
   ],
   "source": [
    "# Examine # of parameters in model\n",
    "def nparams(model):\n",
    "    print(f\"{sum(p.numel() for p in model.parameters())} ({type(model).__name__})\")\n",
    "\n",
    "nparams(model)\n",
    "nparams(model.encoder)\n",
    "nparams(model.feature_extractor)\n",
    "nparams(model.post_extract_proj)\n",
    "nparams(model.layer_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46622a96-1797-45d0-b9d1-b78962acab97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8389760 (Sequential)\n",
      "12596224 (TransformerSentenceEncoderLayer)\n"
     ]
    }
   ],
   "source": [
    "nparams(model.encoder.pos_conv)\n",
    "nparams(model.encoder.layers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60734528-7d5f-4caf-a181-09a289aa1ada",
   "metadata": {},
   "source": [
    "## Examine inference speed(large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6143c56-e0c5-4c8e-b1df-35baaacc0f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 166960])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get one audio sample from librispeech\n",
    "import torchaudio\n",
    "test_data = torchaudio.datasets.LIBRISPEECH(\"../\", \"test-clean\", download=True)\n",
    "sample = test_data[0][0]\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43910951-07b1-49d1-8903-bef94310cf17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2b1f9927784c0794ac09843b924daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 1: 0.2549955487251282\n",
      "Checkpoint 2: 0.0022010564804077148\n",
      "Checkpoint 3: 0.055412154197692874\n",
      "Checkpoint 4: 0.12740011692047118\n",
      "Checkpoint 5: 1.4390701627731324\n",
      "Whole inference time: 1.7516789221763611\n"
     ]
    }
   ],
   "source": [
    "# Compare inference time of each component\n",
    "import time\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sample = test_data[0][0]\n",
    "\n",
    "ckpt1 = []\n",
    "ckpt2 = []\n",
    "ckpt3 = []\n",
    "ckpt4 = []\n",
    "ckpt5 = []\n",
    "whole = []\n",
    "\n",
    "pbar = tqdm(range(100))\n",
    "\n",
    "for i in pbar:\n",
    "    start_time = time.time()\n",
    "    #conv feature extractor\n",
    "    features = model.feature_extractor(sample)\n",
    "    features = features.transpose(1, 2)\n",
    "    features = model.layer_norm(features)\n",
    "    ckpt1_time = time.time()\n",
    "    ckpt1.append(ckpt1_time - start_time)\n",
    "\n",
    "    #post extract proj\n",
    "    features = model.post_extract_proj(features)\n",
    "    ckpt2_time = time.time()\n",
    "    ckpt2.append(ckpt2_time - ckpt1_time)\n",
    "\n",
    "    #conv position embedding\n",
    "    x_conv = model.encoder.pos_conv(features.transpose(1, 2))\n",
    "    x_conv = x_conv.transpose(1, 2)\n",
    "    x = features + x_conv\n",
    "    x = model.encoder.layer_norm(x)\n",
    "    x = F.dropout(x, p=0.1, training=True)\n",
    "    x = x.transpose(0, 1)\n",
    "    ckpt3_time = time.time()\n",
    "    ckpt3.append(ckpt3_time - ckpt2_time)\n",
    "\n",
    "    #Transformer Layers\n",
    "    for i, layer in enumerate(model.encoder.layers):\n",
    "        x, z = layer(x)\n",
    "        if i==1:\n",
    "            ckpt4_time = time.time()\n",
    "            ckpt4.append(ckpt4_time - ckpt3_time)\n",
    "    x = x.transpose(0, 1)\n",
    "    ckpt5_time = time.time()\n",
    "    ckpt5.append(ckpt5_time - ckpt3_time)\n",
    "    whole.append(ckpt5_time - start_time)\n",
    "    \n",
    "print(f\"Checkpoint 1: {np.mean(ckpt1)}\")\n",
    "print(f\"Checkpoint 2: {np.mean(ckpt2)}\")\n",
    "print(f\"Checkpoint 3: {np.mean(ckpt3)}\")\n",
    "print(f\"Checkpoint 4: {np.mean(ckpt4)}\")\n",
    "print(f\"Checkpoint 5: {np.mean(ckpt5)}\")\n",
    "print(f\"Whole inference time: {np.mean(whole)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b30817a-0832-423c-b763-66a53254a20f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
